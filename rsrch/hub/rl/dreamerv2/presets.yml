atari:
  wm:
    rssm:
      deter_size: 600
      hidden_size: 600
    opt.lr: 2e-4
    coef: { kl: 0.1, term: 5.0 }
  ac:
    gamma: 0.999
    actor_ent: 1e-3
    opt.actor.lr: 4e-5
    opt.critic.lr: 1e-4
  total: { n: 200e6, of: env_step }
  prefill: { n: 200e3, of: env_step }
  trainer:
    basic:
      sched: { n: 64, of: env_step }
    iterative:
      wm_sched: { n: 64, of: env_step }
      ac_sched: { n: 64, of: env_step }
atari_der:
  trainer:
    steps.train_every: { n: 16, of: env_step }
debug:
  prefill.n: 20e3
  trainer:
    mode: iterative
    iterative:
      enable_for_wm: true
      wm_sched: { n: 50e3, of: env_step }
