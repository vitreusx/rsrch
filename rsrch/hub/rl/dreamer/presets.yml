sac:
  base:
    wm.loader: ~
    rl:
      type: sac
      loader: real_rl
      sac:
        critic:
          polyak:
            every: 1
            tau: 0.995
        num_q: 2
        gamma: 0.99
        alpha:
          adaptive: true
          init_value: 1e-3
          amp_rate: 1e-3
          decay_rate: 1e-3
          min_ent: auto
        clip_grad: ~
  mujoco:
    $extends: [base]
    env:
      type: gym
      gym:
        env_id: Humanoid-v4
    data:
      capacity: 1e6
      loaders.real_rl:
        batch_size: 256
        slice_len: 2
    train.num_envs: 1
    val.num_envs: 4
    rl.sac:
      actor:
        opt:
          type: adam
          lr: 3e-4
          eps: 1e-5
        opt_ratio: 2
        encoder:
          type: box
          box:
            hidden: 256
            layers: 2
            act: relu
      critic:
        opt:
          type: adam
          lr: 1e-3
          eps: 1e-5
        encoder:
          type: box
          box:
            hidden: 256
            layers: 2
            act: relu
    profile.functions: [do_rl_opt_step]
    stages:
      - env_rollout:
          until: 5e3
          mode: rand
      - train_loop:
          until: 1e6
          tasks:
            - do_rl_opt_step
            - do_env_step
  atari:
    $extends: [base]
    rl.sac:
      actor:
        opt:
          type: adam
          lr: 3e-4
        opt_ratio: 1
        encoder:
          type: sac_image
      critic:
        opt:
          type: adam
          lr: 3e-4
        encoder:
          type: sac_image
ppo:
  base:
    rl.type: ppo
    rl.ppo:
      encoder:
        type: ppo
      actor_dist:
        type: auto
        beta:
          min_std: 0.1
      adv_norm: true
      clip_vloss: true
      vf_coeff: 0.5
      clip_grad: 0.5
      gamma: 0.99
      gae_lambda: 0.95
      share_encoder: false
  mujoco:
    $extends: [base]
    rl.ppo:
      update_epochs: 10
      update_batch: 64
      opt:
        type: adam
        lr: 3e-4
      ent_coeff: 0.0
      clip_coeff: 0.2
  atari:
    $extends: [base]
    rl.ppo:
      update_epochs: 4
      update_batch: 256
      opt:
        type: adam
        lr: 2.5e-4
      ent_coeff: 1e-2
      clip_coeff: 0.1

dreamer:
  base:
    wm.type: dreamer
    rl.type: a2c

  atari:
    $extends: [base]
    wm.dreamer:
      rssm:
        deter_size: 600
        hidden_size: 600
        stoch: { num_tokens: 32, vocab_size: 32 }
      opt.lr: 2e-4
      coef: { kl: 0.1, term: 5.0 }
    rl.a2c:
      gamma: 0.999
      actor_ent: 1e-3
      actor_grad: reinforce
      opt.actor.lr: 4e-5
      opt.critic.lr: 1e-4

  dmc:
    $extends: [base]
    wm.dreamer:
      reward_fn: id
      rssm:
        hidden_size: 200
        deter_size: 200
      opt.lr: 3e-4
      kl.free: 1.0
      encoder:
        type: auto
        dict.keys: [orientations, velocity]
      decoders:
        obs:
          type: auto
          dict.keys: [orientations, velocity]
        term:
          type: const
          const.value: false
    rl.a2c:
      opt.actor.lr: 8e-5
      opt.critic.lr: 8e-5
      actor_ent: 1e-4

atari:
  base:
    env:
      type: atari
      atari.env_id: Pong

  for_100k:
    env.atari.sticky: false

dmc:
  proprio:
    data.buffer.prioritize_ends: false
    env:
      type: dmc
      dmc:
        domain: walker
        task: walk
        obs_type: proprio
        frame_skip: 2

  vision:
    data.buffer.prioritize_ends: false
    env:
      type: dmc
      dmc:
        domain: walker
        task: walk
        obs_type: visual
        frame_skip: 2
        render_size: [64, 64]

exp:
  sanity_check:
    $extends: [dreamer.atari, atari.base]
    stages:
      - env_rollout:
          until: 200e3
          mode: rand
      - train_loop:
          until: 5e6
          tasks:
            - do_val_epoch: ~
              every: 1e6
            - do_opt_step: ~
              every:
                n: 64
                accumulate: true
            - do_env_step

  mono_rv:
    $extends: [dreamer.atari, atari.base]
    _freq: 64
    data.val_frac: 0.15
    stages:
      - env_rollout:
          until: ${max(3e3 * _freq, 20e3)}
          mode: rand
      - train_loop:
          until: ${max(1e5 * _freq, 400e3)}
          tasks:
            - do_val_epoch: ~
              every: ${max(5e3 * _freq, 20e3)}
            - do_val_step: ~
              every: { n: 16, of: wm_opt_step }
            - do_opt_step: ~
              every:
                n: ${_freq}
                accumulate: true
            - do_env_step

  at_400k:
    $extends: [dreamer.atari, atari.base]
    _freq: 64
    _prefill: 20e3
    data.val_frac: 0.15
    stages:
      - env_rollout:
          until: ${_prefill}
          mode: rand
      - train_loop:
          until: 400e3
          tasks:
            - do_val_epoch: ~
              every: 20e3
            - do_val_step: ~
              every: { n: 16, of: wm_opt_step }
            - do_opt_step: ~
              every:
                n: ${_freq}
                accumulate: true
            - do_env_step

  export:
    $extends: [dreamer.atari, atari.base]
    _freq: 64
    stages:
      - env_rollout:
          until: ${3e3 * _freq}
          mode: rand
      - train_loop:
          until: ${1e5 * _freq}
          tasks:
            - do_val_epoch: ~
              every: ${5e3 * _freq}
            - do_opt_step: ~
              every:
                n: ${_freq}
                accumulate: true
            - do_env_step
      - save_ckpt:
          full: false
          tag: final
