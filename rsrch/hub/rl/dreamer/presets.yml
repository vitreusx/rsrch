atari:
  base:
    env:
      type: atari
      atari.env_id: Pong
    wm.dreamer:
      rssm:
        deter_size: 600
        hidden_size: 600
        stoch: { num_tokens: 32, vocab_size: 32 }
      opt.lr: 2e-4
      coef: { kl: 0.1, term: 5.0 }
    rl.a2c:
      gamma: 0.999
      actor_ent: 1e-3
      actor_grad: reinforce
      opt.actor.lr: 4e-5
      opt.critic.lr: 1e-4

  base_100k:
    $extends: [base]
    env.atari.sticky: false

  train:
    _freq: 64
    stages:
      - env_rollout:
          until: ${3e3 * _freq}
          mode: rand
      - train_loop:
          until: ${1e5 * _freq}
          tasks:
            - do_val_epoch: ~
              every: ${5e3 * _freq}
            - do_opt_step: ~
              every:
                n: ${_freq}
                accumulate: true
            - do_env_step

  debug:
    stages:
      - env_rollout:
          until: 25e3
          mode: rand
      - train_loop:
          until: 100e3
          tasks:
            - do_val_epoch: ~
              every: 10e3
            - do_opt_step: ~
              every: 64
            - do_env_step

  train_val:
    _freq: 64
    data.val_frac: 0.15
    stages:
      - env_rollout:
          until: ${max(3e3 * _freq, 20e3)}
          mode: rand
      - train_loop:
          until: ${max(1e5 * _freq, 400e3)}
          tasks:
            - do_val_epoch: ~
              every: ${max(5e3 * _freq, 20e3)}
            - do_val_step: ~
              every: { n: 16, of: wm_opt_step }
            - do_opt_step: ~
              every:
                n: ${_freq}
                accumulate: true
            - do_env_step

  primed_wm:
    _freq: 8
    data:
      val_frac: 0.15
    stages:
      - env_rollout:
          until: 40e3
          mode: rand
      - train_wm:
          stop_criteria: { rel_patience: 0.5, min_steps: 10e3 }
          val_every: 1024
          val_on_loss_improv: 0.2
          max_val_batches: 128
      - train_loop:
          until: 400e3
          tasks:
            - do_val_epoch: ~
              every: 40e3
            - do_val_step: ~
              every: { n: 16, of: wm_opt_step }
            - do_opt_step: ~
              every:
                n: ${_freq}
                accumulate: true
            - do_env_step

  rl_conv_test:
    data:
      val_frac: 0.15
    stages:
      - env_rollout:
          until: 200e3
          mode: rand
      - train_wm:
          stop_criteria: { rel_patience: 0.5, min_steps: 10e3 }
          val_every: 1024
          val_on_loss_improv: 0.2
          max_val_batches: 128
      - train_loop:
          until: { n: 100e3, of: rl_opt_step }
          tasks:
            - do_val_epoch:
                step: rl_opt_step
              every: 5e3
            - do_val_step: ~
              every: { n: 16, of: rl_opt_step }
            - do_rl_opt_step

dmc:
  base:
    wm.dreamer:
      reward_fn: id
      rssm:
        hidden_size: 200
        deter_size: 200
      opt.lr: 3e-4
      kl.free: 1.0
      encoder:
        type: auto
        dict.keys: [orientations, velocity]
      decoders:
        obs:
          type: auto
          dict.keys: [orientations, velocity]
        term:
          type: const
          const.value: false
    rl.a2c:
      opt.actor.lr: 8e-5
      opt.critic.lr: 8e-5
      actor_ent: 1e-4
    data.buffer.prioritize_ends: false

  proprio:
    env:
      type: dmc
      dmc:
        domain: walker
        task: walk
        obs_type: proprio
        frame_skip: 2

  vision:
    env:
      type: dmc
      dmc:
        domain: walker
        task: walk
        obs_type: visual
        frame_skip: 2
        render_size: [64, 64]

  train:
    stages:
      - env_rollout:
          until: 10e3 # 2e3
          mode: rand
      - train_loop:
          until: { n: 100, of: wm_opt_step }
          tasks: [do_opt_step]
      - train_loop:
          until: 100e6
          tasks:
            - do_opt_step: ~
              every: { n: 5, of: agent_step }
            - do_env_step
