atari:
  base:
    env:
      type: atari
      atari.env_id: Pong
    wm.dreamer:
      rssm:
        deter_size: 600
        hidden_size: 600
        stoch: { num_tokens: 32, vocab_size: 32 }
      opt.lr: 2e-4
      coef: { kl: 0.1, term: 5.0 }
    ac.ref:
      gamma: 0.999
      actor_ent: 1e-3
      actor_grad: reinforce
      opt.actor.lr: 4e-5
      opt.critic.lr: 1e-4

  train:
    _freq: 64
    stages:
      - env_rollout:
          until: ${3e3 * _freq}
          mode: rand
      - train_loop:
          until: ${1e5 * _freq}
          tasks:
            - do_val_epoch: ~
              every: ${5e3 * _freq}
            - do_opt_step: ~
              every: ${_freq}
            - do_env_step

  train_val:
    _freq: 64
    data.val_frac: 0.1
    stages:
      - env_rollout:
          until: ${max(3e3 * _freq, 20e3)}
          mode: rand
      - train_loop:
          until: ${max(1e5 * _freq, 400e3)}
          tasks:
            - do_val_epoch: ~
              every: ${max(5e3 * _freq, 20e3)}
            - do_val_step: ~
              every: { n: 16, of: wm_opt_step }
            - do_opt_step: ~
              every: ${_freq}
            - do_env_step

  primed_wm:
    _freq: 64
    data.val_frac: 0.1
    stages:
      - env_rollout:
          until: ${3e3 * _freq}
          mode: rand
      - train_loop:
          until: ${1e5 * _freq}
          tasks:
            - train_wm:
                stop_criteria: { patience: 2048 }
                val_every: 512
                max_val_batches: 64
                reset: true
              every: ${3e3 * _freq}
            - do_val_epoch: ~
              every: ${5e3 * _freq}
            - do_val_step: ~
              every: { n: 16, of: wm_opt_step }
            - do_opt_step: ~
              every: ${_freq}
            - do_env_step

  opt_400k_v1:
    _opt_every: 4
    data:
      val_frac: 0.1
    stages:
      - env_rollout:
          until: 40e3
          mode: rand
      - train_loop:
          until: 400e3
          tasks:
            - train_wm:
                stop_criteria: { patience: 2048 }
                val_every: 512
                max_val_batches: 64
                reset: true
              every: 40e3
            - do_opt_step: ~
              every: ${_opt_every}
            - do_val_step: ~
              every: { n: 16, of: wm_opt_step }
            - do_env_step

  ac_conv_test:
    data:
      val_frac: 0.1
    stages:
      - env_rollout:
          until: 100e3
          mode: rand
      - train_wm:
          stop_criteria: { patience: 2048 }
          val_every: 512
          max_val_batches: 64
          reset: true
      - train_loop:
          until: { n: 100e3, of: ac_opt_step }
          tasks:
            - do_ac_opt_step

dmc:
  base:
    wm.dreamer:
      reward_fn: id
      rssm:
        hidden_size: 200
        deter_size: 200
      opt.lr: 3e-4
      kl.free: 1.0
      encoder:
        type: auto
        dict.keys: [orientations, velocity]
      decoders:
        obs:
          type: auto
          dict.keys: [orientations, velocity]
        term:
          type: const
          const.value: false
    ac.ref:
      opt.actor.lr: 8e-5
      opt.critic.lr: 8e-5
      actor_ent: 1e-4
    data.buffer.prioritize_ends: false

  proprio:
    env:
      type: dmc
      dmc:
        domain: walker
        task: walk
        obs_type: proprio
        frame_skip: 2

  vision:
    env:
      type: dmc
      dmc:
        domain: walker
        task: walk
        obs_type: visual
        frame_skip: 2
        render_size: [64, 64]

  train:
    stages:
      - env_rollout:
          until: 10e3 # 2e3
          mode: rand
      - train_loop:
          until: { n: 100, of: wm_opt_step }
          tasks: [do_opt_step]
      - train_loop:
          until: 100e6
          tasks:
            - do_opt_step: ~
              every: { n: 5, of: agent_step }
            - do_env_step

default:
  $extends: [atari.base, atari.opt_400k_v1]
