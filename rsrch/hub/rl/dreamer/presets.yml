no_model:
  wm.type: ~
  wm.loader: ~
  rl.loader: real_rl

sac:
  base:
    $extends: [no_model]
    rl:
      type: sac
      sac:
        num_qf: 2
        gamma: 0.99
        alpha:
          adaptive: true
          value: 1.0
          target: auto
          opt:
            type: adam
            lr: ${qf.opt.lr}
        clip_grad: ~
        actor:
          opt:
            type: adam
            eps: 1e-5
        qf:
          opt:
            type: adam
            eps: 1e-5
    profile.functions: [do_rl_opt_step]
  mujoco:
    $extends: [base]
    env:
      type: gym
      gym:
        env_id: HalfCheetah-v4
    data:
      capacity: 1e6
      loaders.real_rl:
        batch_size: 256
        slice_len: 2
    rl.sac:
      actor:
        encoder:
          type: box
          box:
            hidden: 256
            layers: 2
            act: relu
        opt.lr: 3e-4
      qf:
        polyak:
          every: 1
          tau: 0.995
        encoder:
          type: box
          box:
            hidden: 256
            layers: 2
            act: relu
        opt.lr: 1e-3
    stages:
      - env_rollout:
          until: { n: 5e3, of: agent_step }
          mode: rand
      - train_loop:
          until: { n: 1e6, of: agent_step }
          tasks:
            - do_rl_opt_step
            - do_env_step
  atari:
    $extends: [base]
    env:
      type: atari
      atari:
        env_id: Breakout
        screen_size: 84
        stack_num: 4
        term_on_life_loss: true
    data:
      capacity: 1e6
      loaders.real_rl:
        batch_size: 64
        slice_len: 2
    rl.sac:
      actor:
        encoder:
          type: sac_image
        opt:
          lr: 3e-4
          eps: 1e-4
      qf:
        polyak:
          every: 2000
          tau: 0.0
        encoder:
          type: sac_image
        opt:
          lr: 3e-4
          eps: 1e-4
    stages:
      - env_rollout:
          until: { n: 20e3, of: agent_step }
          mode: rand
      - train_loop:
          until: { n: 5e6, of: agent_step }
          tasks:
            - do_val_epoch: ~
              every: 100e3
            - do_rl_val_step: ~
              every: { n: 16, of: rl_opt_step }
            - do_rl_opt_step: ~
              every: { n: 4, of: agent_step }
            - do_env_step

ppo:
  base:
    $extends: [no_model]
    rl:
      type: ppo
      loader: on_policy
      ppo:
        encoder:
          type: ppo
        actor_dist:
          type: auto
        adv_norm: true
        clip_vloss: true
        vf_coeff: 0.5
        clip_grad: 0.5
        gamma: 0.99
        gae_lambda: 0.95
        share_encoder: true
    data.loaders.on_policy:
      min_seq_len: 16
  mujoco:
    $extends: [base]
    env:
      type: gym
      gym.env_id: Humanoid-v4
    rl.ppo:
      update_epochs: 10
      update_batch: 64
      opt:
        type: adam
        lr: 3e-4
      clip_coeff: 0.2
      ent_coeff: 0.0
    train.num_envs: 1
    data.loaders.on_policy:
      steps_per_batch: 2048
    stages:
      - train_loop:
          until: 1e6
          tasks: [do_rl_opt_step]
  atari:
    $extends: [base]
    env:
      type: atari
      atari:
        env_id: Pong
        screen_size: 84
        stack_num: 4
        term_on_life_loss: true
    rl.ppo:
      update_epochs: 4
      update_batch: 256
      opt:
        type: adam
        lr: 2.5e-4
        eps: 1e-5
      clip_coeff: 0.1
      ent_coeff: 1e-2
    train.num_envs: 8
    data.loaders.on_policy:
      steps_per_batch: 1024
    stages:
      - train_loop:
          until: 40e6
          tasks: [do_rl_opt_step]

dreamer:
  base:
    wm:
      type: dreamer
      loader: real_wm
    rl:
      type: a2c
      loader: dream_rl
  atari:
    $extends: [base]
    wm.dreamer:
      rssm:
        deter_size: 600
        hidden_size: 600
        stoch: { num_tokens: 32, vocab_size: 32 }
      opt.lr: 2e-4
      coef: { kl: 0.1, term: 5.0 }
    rl.a2c:
      gamma: 0.999
      actor_ent: 1e-3
      actor_grad: reinforce
      opt.actor.lr: 4e-5
      opt.critic.lr: 1e-4
  dmc:
    $extends: [base]
    wm.dreamer:
      reward_fn: id
      rssm:
        hidden_size: 200
        deter_size: 200
      opt.lr: 3e-4
      kl.free: 1.0
      encoder:
        type: auto
        dict.keys: [orientations, velocity]
      decoders:
        obs:
          type: auto
          dict.keys: [orientations, velocity]
        term:
          type: const
          const.value: false
    rl.a2c:
      opt.actor.lr: 8e-5
      opt.critic.lr: 8e-5
      actor_ent: 1e-4

atari:
  base:
    env:
      type: atari
      atari.env_id: Pong
  v4:
    env.atari:
      repeat_action_probability: 0.0
  for_100k:
    env.atari.sticky: false

dmc:
  proprio:
    data.loaders.real_wm.prioritize_ends: false
    env:
      type: dmc
      dmc:
        domain: walker
        task: walk
        obs_type: proprio
        frame_skip: 2
  vision:
    data.loaders.real_wm.prioritize_ends: false
    env:
      type: dmc
      dmc:
        domain: walker
        task: walk
        obs_type: visual
        frame_skip: 2
        render_size: [64, 64]

exp:
  sanity_check:
    $extends: [dreamer.atari, atari.base]
    stages:
      - env_rollout:
          until: 200e3
          mode: rand
      - train_loop:
          until: 5e6
          tasks:
            - do_val_epoch: ~
              every: 1e6
            - do_opt_step: ~
              every:
                n: 64
                accumulate: true
            - do_env_step
  ratio_val_test:
    $extends: [dreamer.atari, atari.base]
    _freq: 64
    data.val_frac: 0.15
    stages:
      - env_rollout:
          until: ${max(3e3 * _freq, 20e3)}
          mode: rand
      - train_loop:
          until: ${max(1e5 * _freq, 400e3)}
          tasks:
            - do_val_epoch: ~
              every: ${max(5e3 * _freq, 20e3)}
            - do_val_step: ~
              every: { n: 16, of: wm_opt_step }
            - do_opt_step: ~
              every:
                n: ${_freq}
                accumulate: true
            - do_env_step
      - save_ckpt:
          full: false
          tag: final
  at_400k:
    $extends: [dreamer.atari, atari.base]
    _freq: 64
    _prefill: 20e3
    data.val_frac: 0.15
    stages:
      - env_rollout:
          until: ${_prefill}
          mode: rand
      - train_loop:
          until: 400e3
          tasks:
            - do_val_epoch: ~
              every: 20e3
            - do_val_step: ~
              every: { n: 16, of: wm_opt_step }
            - do_opt_step: ~
              every:
                n: ${_freq}
                accumulate: true
            - do_env_step
      - save_ckpt:
          full: false
          tag: final
  export:
    $extends: [dreamer.atari, atari.base]
    _freq: 64
    stages:
      - env_rollout:
          until: ${3e3 * _freq}
          mode: rand
      - train_loop:
          until: ${1e5 * _freq}
          tasks:
            - do_val_epoch: ~
              every: ${5e3 * _freq}
            - do_opt_step: ~
              every:
                n: ${_freq}
                accumulate: true
            - do_env_step
      - save_ckpt:
          full: false
          tag: final
  use_expert:
    $extends: [dreamer.atari, atari.base]
    _ckpt_path: ~
    _data_path: ~
    stages:
      - load_ckpt:
          path: ${_ckpt_path}
      - do_val_epoch: ~
      - env_rollout:
          until: 400e3
          mode: train
      - save_buf:
          path: ${_data_path}
  offline:
    $extends: [dreamer.atari, atari.base]
    _data_path: ~
    def_step: wm_opt_step
    stages:
      - load_buf:
          path: ${_data_path}
      - train_loop:
          until: { n: 25e3, of: wm_opt_step }
          tasks:
            - do_val_epoch: ~
              every: 1e3
            - do_val_step: ~
              every: { n: 16, of: wm_opt_step }
            - do_opt_step
  with_expert_data:
    $extends: [dreamer.atari, atari.base]
    _freq: 64
    _data_path: ~
    stages:
      - load_buf:
          path: ${_data_path}
      - train_loop:
          until: 100e3
          tasks:
            - do_val_epoch: ~
              every: 10e3
            - do_val_step: ~
              every: { n: 16, of: wm_opt_step }
            - do_opt_step: ~
              every:
                n: ${_freq}
                accumulate: true
            - do_env_step
  sr_sac:
    $extends: [no_model, sac.mujoco, dmc.proprio]
    _updates_per_step: 1
    _reset_interval: 2.56e6
    profile.functions: [do_rl_opt_step]
    env:
      type: dmc
      dmc:
        domain: walker
        task: walk
        frame_skip: 1
        obs_type: proprio_flat
    rl.sac:
      opt:
        $replace: true
        type: adam
        lr: 3e-4
        betas: [0.9, 0.999]
        eps: 1.5e-4
      actor:
        encoder:
          type: box
          box:
            hidden: 256
            layers: 2
            act: relu
      qf:
        encoder: ${..actor.encoder}
        polyak:
          every: 1
          tau: 0.995
      gamma: 0.99
    data:
      capacity: 1e6
      loaders.real_rl:
        batch_size: 256
        slice_len: 2
    stages:
      - env_rollout:
          until: 5e3
          mode: rand
      - train_loop:
          until: 500e3
          tasks:
            - do_val_epoch: ~
              every: 50e3
            - do_rl_val_step: ~
              every: { n: 16, of: rl_opt_step }
            - reset_rl: ~
              every:
                n: ${_reset_interval}
                of: rl_opt_step
            - do_rl_opt_step:
                n: ${_updates_per_step}
            - do_env_step
  dreamer_sac:
    $extends: [ratio_val_test]
    rl:
      type: sac
      sac:
        opt:
          type: adam
          eps: 1e-5
          actor: { lr: 3e-4 }
          qf: { lr: 1e-5 }
        actor:
          encoder:
            type: box
            box:
              hidden: 256
              layers: 2
              act: relu
        qf:
          encoder:
            type: box
            box:
              hidden: 256
              layers: 2
              act: relu
          polyak:
            every: 1
            tau: 0.995
        num_q: 2
        gamma: 0.99
        clip_grad: ~
        alpha:
          adaptive: true
          init_value: 1e-3
          amp_rate: 1e-3
          decay_rate: 1e-3
          min_ent: auto
