_:
  act: elu
  norm: none
  conv: &conv
    depth: 48
    kernels: [4, 4, 4, 4]
    act: ${_.act}
    norm: ${_.norm}
  conv_t: &conv_t
    depth: ${_.conv.depth}
    kernels: [5, 5, 6, 6]
    act: ${_.act}
    norm: ${_.norm}
  mlp: &mlp
    hidden: 400
    layers: 4
    act: ${_.act}
    norm: ${_.norm}
  mse: &mse
    type: mse
    mse:
      init: tf

run:
  dir: ~
  interactive: true
  create_commit: true

repro:
  seed: 42
  determinism: sufficient
device: cuda
compute_dtype: float16
def_step: env_step
debug:
  detect_anomaly: false
profile:
  enabled: false
  schedule: { wait: 5, warmup: 1, active: 3, repeat: 4 }
  functions: [do_opt_step, do_env_step]
mode: train

env:
  type: atari
  atari:
    env_id: Pong
    screen_size: 64
    frame_skip: 4
    obs_type: grayscale
    noop_max: 30
    fire_reset: true
    term_on_life_loss: false
    repeat_action_probability: 0.25
    time_limit: 108e3
    stack_num: ~

data:
  capacity: 2e6
  val_frac: 0.0
  loaders:
    dreamer_wm:
      batch_size: 16
      slice_len: 50
      subseq_len: [50, 50]
      prioritize_ends: true
      ongoing: false
    dreamer_rl:
      batch_size: ${dreamer_wm.batch_size * dreamer_wm.slice_len}
      slice_len: 16
    slices_rl: {}
    on_policy: {}

train:
  num_envs: 1
  agent_noise: 0.0
val:
  num_envs: 4
  agent_noise: 0.0

wm:
  type: dreamer
  loader: real_wm
  dreamer:
    rssm:
      ensemble: 1
      deter_size: 1024
      stoch:
        type: discrete
        num_tokens: 32
        vocab_size: 32
      act: ${_.act}
      norm: ${_.norm}
      hidden_size: 1024
      jit: true
    opt:
      type: adam_w
      lr: 1e-4
      eps: 1e-5
      weight_decay: "${1e-6/lr}"
    kl:
      free: 0.0
      forward: false
      balance: 0.8
      free_avg: true
    reward_fn: tanh
    clip_rew: [-1.0, 1.0]
    coef: {}
    clip_grad: 1.0e2
    encoder:
      type: dreamer_auto
      box: *mlp
      image: *conv
    decoders:
      obs:
        type: dreamer_auto
        box:
          <<: [*mlp]
          dist: *mse
        image:
          <<: [*conv_t]
          dist: *mse
      reward:
        type: dreamer_box
        dreamer_box:
          <<: [*mlp]
          dist: *mse
      term:
        type: dreamer_discrete
        dreamer_discrete:
          <<: [*mlp]
          dist:
            type: bern
            bern:
              init: tf

rl:
  type: a2c
  loader: dream_rl
  a2c:
    actor:
      encoder:
        type: dreamer_box
        dreamer_box: *mlp
      dist:
        type: auto
        box:
          min_std: 0.1
          init: tf
        one_hot:
          init: tf
    critic:
      encoder:
        type: dreamer_box
        dreamer_box: *mlp
      dist: *mse
    target_critic: { every: 100 }
    gamma: 0.99
    gae_lambda: 0.95
    actor_grad: auto
    actor_grad_mix: 0.1
    alpha:
      adaptive: true
      value: 2.5e-3
      target: auto
      auto_coefs: { disc: 0.2, cont: 5e-2 }
      opt:
        type: adam
        lr: 1e-3
        eps: 1e-4
    coef: {}
    opt:
      type: adam_w
      actor: { lr: 8e-5, weight_decay: "${1e-6/lr}" }
      critic: { lr: 2e-4, weight_decay: "${1e-6/lr}" }
      eps: 1e-5
    clip_grad: 1.0e2
    rew_norm:
      momentum: 1.0
      scale: 1.0
      eps: 1e-8
  ppo: ~
  sac: ~

stages:
  - prefill:
      until: 10e3
  - train_loop:
      until: 100e6
      tasks:
        - do_opt_step: ~
          every: 5
        - do_env_step
