{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TinyImagenet' from 'rsrch.datasets.tiny_imagenet' (/home/j.bednarz/projects/rsrch/rsrch/datasets/tiny_imagenet.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrsrch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvqvae\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpixelcnn\u001b[39;00m \u001b[39mimport\u001b[39;00m PixelCNN\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrsrch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m font_awesome, tiny_imagenet\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m Categorical\n",
      "File \u001b[0;32m~/projects/rsrch/rsrch/datasets/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfont_awesome\u001b[39;00m \u001b[39mimport\u001b[39;00m FontAwesome\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtiny_imagenet\u001b[39;00m \u001b[39mimport\u001b[39;00m TinyImagenet\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TinyImagenet' from 'rsrch.datasets.tiny_imagenet' (/home/j.bednarz/projects/rsrch/rsrch/datasets/tiny_imagenet.py)"
     ]
    }
   ],
   "source": [
    "from rsrch.vqvae.pixelcnn import PixelCNN\n",
    "from rsrch.datasets import font_awesome, tiny_imagenet\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "from torch import Tensor\n",
    "import rsrch.utils.data as data\n",
    "import torchvision.transforms.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        self.train_batch_size = 16\n",
    "        self.val_batch_size = 16\n",
    "        self.num_workers = 4\n",
    "        self.num_epochs = 32\n",
    "        self.val_every = 16\n",
    "\n",
    "    def setup(self):\n",
    "        self.setup_data()\n",
    "        self.setup_model()\n",
    "\n",
    "    def setup_data(self):\n",
    "        # self.setup_font_awesome()\n",
    "        self.setup_tiny_imagenet()\n",
    "\n",
    "    def setup_font_awesome(self):\n",
    "        ds_root = Path(__file__).parent / \"../datasets/font-awesome\"\n",
    "\n",
    "        self.ds = font_awesome.FontAwesome(\n",
    "            root=ds_root,\n",
    "            color=\"black\",\n",
    "            resolution=32,\n",
    "        )\n",
    "\n",
    "        self._in_channels = 1\n",
    "        self._value_range = 2\n",
    "\n",
    "        self.train_ds, self.val_ds = data.random_split(self.ds, [0.8, 0.2])\n",
    "\n",
    "        def val_transform(item: font_awesome.Item) -> Tensor:\n",
    "            image = item.image.split()[-1]\n",
    "            image = F.to_tensor(image)\n",
    "            image = (image > 0.5).float()\n",
    "            image = F.center_crop(image, (32, 32))\n",
    "            return image\n",
    "\n",
    "        self.val_ds = self.val_ds.map(val_transform)\n",
    "        self.val_loader = data.DataLoader(\n",
    "            dataset=self.val_ds,\n",
    "            batch_size=self.val_batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "        def train_transform(item: font_awesome.Item) -> Tensor:\n",
    "            image = val_transform(item)\n",
    "            return image\n",
    "\n",
    "        self.train_ds = self.train_ds.map(train_transform)\n",
    "        self.train_loader = data.DataLoader(\n",
    "            dataset=self.train_ds,\n",
    "            batch_size=self.train_batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def setup_tiny_imagenet(self):\n",
    "        ds_root = Path(__file__).parent / \"../datasets/tiny-imagenet-200\"\n",
    "\n",
    "        self.train_ds = tiny_imagenet.TinyImageNet(root=ds_root, split=\"train\")\n",
    "\n",
    "        self.val_ds = tiny_imagenet.TinyImageNet(\n",
    "            root=ds_root,\n",
    "            split=\"val\",\n",
    "        )\n",
    "\n",
    "        self._in_channels = 3\n",
    "        self._value_range = 256\n",
    "\n",
    "        def val_transform(item: font_awesome.Item) -> Tensor:\n",
    "            image = item.image\n",
    "            image = image.convert(\"RGB\")\n",
    "            image = F.center_crop(image, (32, 32))\n",
    "            image = F.to_tensor(image)\n",
    "            return image\n",
    "\n",
    "        self.val_ds = self.val_ds.map(val_transform)\n",
    "        self.val_loader = data.DataLoader(\n",
    "            dataset=self.val_ds,\n",
    "            batch_size=self.val_batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "        def train_transform(item: font_awesome.Item) -> Tensor:\n",
    "            image = val_transform(item)\n",
    "            return image\n",
    "\n",
    "        self.train_ds = self.train_ds.map(train_transform)\n",
    "        self.train_loader = data.DataLoader(\n",
    "            dataset=self.train_ds,\n",
    "            batch_size=self.train_batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def setup_model(self):\n",
    "        self.model = PixelCNN(\n",
    "            in_channels=self._in_channels,\n",
    "            hidden_dim=128,\n",
    "            value_range=self._value_range,\n",
    "            kernel_size=7,\n",
    "            num_layers=9,\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.model = self.model.to(self.device)\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for batch in self.train_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                logits = self.model(batch)\n",
    "                pr_dist = Categorical(logits=logits)\n",
    "                targets = self._value_range * batch\n",
    "                targets = targets.clamp(0, self._value_range - 1).long()\n",
    "                loss = -pr_dist.log_prob(targets).sum()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if epoch % self.val_every == 0:\n",
    "                val_loss, num_items = 0, 0\n",
    "                for batch in self.val_loader:\n",
    "                    batch = batch.to(self.device)\n",
    "                    logits = self.model(batch)\n",
    "                    pr_dist = Categorical(logits=logits)\n",
    "                    targets = self._value_range * batch\n",
    "                    targets = targets.clamp(0, self._value_range - 1).long()\n",
    "                    val_loss += -pr_dist.log_prob(targets).sum().item()\n",
    "                    num_items += len(batch)\n",
    "\n",
    "                val_loss = val_loss / num_items\n",
    "                print(f\"val | epoch={epoch}, val_loss={val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m trainer\u001b[39m.\u001b[39mnum_epochs \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m\n\u001b[1;32m      3\u001b[0m trainer\u001b[39m.\u001b[39msetup()\n\u001b[0;32m----> 4\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/projects/rsrch/stuff/pixelcnn.py:128\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_epochs):\n\u001b[1;32m    127\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader:\n\u001b[0;32m--> 128\u001b[0m         batch \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    129\u001b[0m         logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(batch)\n\u001b[1;32m    130\u001b[0m         pr_dist \u001b[39m=\u001b[39m Categorical(logits\u001b[39m=\u001b[39mlogits)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer()\n",
    "trainer.num_epochs = 128\n",
    "trainer.setup()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = trainer.val_ds[0].unsqueeze(0)\n",
    "batch = batch.to(trainer.device)\n",
    "pred = trainer.model.predict(batch, (16, 16))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAAC/0lEQVR4nO3dS46cMAAA0UmU+185Wc2GCJmP4XXiqu1MQ1GyZBpo8/UVEREREREREavx4+kd/L75+acFfz68/Y+nAFpAUwAtoCmAFtBMn2bvzvsjZgsvPwIKoAU0BdACmgJoAc3tafXpeX/E3QNYfgQUQAtoCqAFNAXQAprL06ie/7dcPZDlR0ABtICmAFpAUwAtoDk9fX7a/L/l7AEtPwIKoAU0BdACmgJoAc3hafPT5/8tRw9s+RFQAC2gKYAW0BRAC2j+u+sBW0YHuPwIKIAW0BRAC2gKoAU0w/OA73l/+49Xzwee+h3gnk/nAQMKoAU0BdACmgJoAc2vvT/M+t7/+AIFN/ez/AgogBbQFEALaAqgBTS70+fR79dXv4dvP3d0Hp99HWL5EVAALaApgBbQFEALaP6aHkfz7N58unf/4Oh2z+7nKtvtLj8CCqAFNAXQApoCaAHN6fOA3Q/uMHve7jxgMgXQApoCaAFNAbSAZvf5gE/lex6f9fzC8iOgAFpAUwAtoCmAFtBcPg+YfR/g6n7usvwIKIAW0BRAC2gKoAU0BdACmgJoAU0BtICmAFpAc/u+wN739VnX73vv8MMUQAtoCqAFNAXQApp/7vmA2esZLj8CCqAFNAXQApoCaAHNtPOA2dcF3lp/aPkRUAAtoCmAFtAUQAtoTq8jdHvDO9s/+/9naR2hHQqgBTQF0AKaAmgBzWP3BUbP+b/1fX/E8iOgAFpAUwAtoCmAFtAcfs/QazucvP/eMzSgAFpAUwAtoCmAFtC8/pzg3fcWzGb5EVAALaApgBbQFEALaIbnAbPX73uLo9cdlh8BBdACmgJoAU0BtIDm9G36p+8TvL3ewPIjoABaQFMALaApgBbQXH5cb9b5gF5vaPkRUAAtoCmAFtAUQAtobj+2f3X+nnUd4O4BLD8CCqAFNAXQApoCaAHN9J/vnX1vsV5faPkRUAAtoCmAFtAUQAtoHv8Z/2i9oNF5QO8XeJgCaAFNAbSApgBaICIiIiIiIiLibf4AAGUzw80eFpsAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "image = trainer.val_ds[3]\n",
    "F.to_pil_image(image).resize((256, 256), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAADEUlEQVR4nO3dUU7bQBgAYai4V6+2R+vN2pfy4sq1s7H5InbmBVFiezL6pd04oby9RURERERERMRqvN99gd9PHn+34I+bz//yFEALaAqgBTQF0AKay5fZZ9f9I64WXn4CCqAFNAXQApoCaAHN08vq3ev+Ec8+geUnoABaQFMALaApgBbQTC+jev3fMvtElp+AAmgBTQG0gKYAWkDz8PL5auv/lkef0PITUAAtoCmAFtAUQAtoTi+br77+bzn7xJafgAJoAU0BtICmAFpA8+3uB2w5eoLLT0ABtICmAFpAUwAtoDncB3yu+9sHzu4H7vo9wD2f9gEHFEALaAqgBTQF0AKaj70fXPW6//b/oODJ6yw/AQXQApoCaAFNAbSAZnf5PPv6evZ1+Pa4s+v41fchlp+AAmgBTQG0gKYAWkDzz/J4tM7urad77x+cPe+j15lle97lJ6AAWkBTAC2gKYAW0Dy8D9g9cIer1+32ARdTAC2gKYAW0BRAC2h2Px/wqnyu41d9fmH5CSiAFtAUQAtoCqAFNNP7gKvfB5i9zrMsPwEF0AKaAmgBTQG0gIbfD/h18nE/J487Os/yE1AALaApgBbQFEALaD5m19fx9+v2+EfPc5ax+XoVy09AAbSApgBaQFMALaB5+H7AuEFCXn/5CSiAFtAUQAtoCqAFNKf3AWPyArPHnT3f0fdHLD8BBdACmgJoAU0BtIDmcB8wJk88e9xXs/wEFEALaAqgBTQF0AIa/jnBqxiTxy0/AQXQApoCaAFNAbSA5tvsA44YO/++/AQUQAtoCqAFNAXQApoCaAFNAbSApgBaQFMALaA5/fmA8Z/HvDLj4OfLT0ABtICmAFpAUwAtoPm27wuMk49bfgIKoAU0BdACmgJoAc232QeMyeOWn4ACaAFNAbSApgBaQHPb7w3exbj4fMtPQAG0gKYAWkBTAC2gKYAW0BRAC2gKoAU0BdACmt37AeMLJSTLT0ABtICmAFpAUwAtoHnf+ztBq7D8BBRAC2gKoAU0BdACmgJoAU0BtICmAFpAUwAtoPkDDLArLmRPr9IAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "batch = image.unsqueeze(0).to(trainer.device)\n",
    "pred = trainer.model.predict(batch, (16, 16))[0]\n",
    "for iy, ix, ic in np.ndindex((32, 32, 1)):\n",
    "    if (iy, ix) >= (16, 16):\n",
    "        pred[ic,iy,ix] = 0.25 + 0.5 * pred[ic,iy,ix]\n",
    "F.to_pil_image(pred).resize((256, 256), 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
