{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch: train_episodes=32, mean=-170.76, std=81.57, min=-364.34, max=-79.27\n",
      "val epoch: train_episodes=64, mean=-151.68, std=31.14, min=-206.74, max=-90.44\n",
      "val epoch: train_episodes=96, mean=-132.62, std=39.42, min=-227.71, max=-33.53\n",
      "val epoch: train_episodes=128, mean=-170.18, std=44.53, min=-255.26, max=-64.48\n",
      "val epoch: train_episodes=160, mean=-123.37, std=19.31, min=-163.50, max=-89.36\n",
      "val epoch: train_episodes=192, mean=-115.79, std=47.88, min=-181.86, max=33.79\n",
      "val epoch: train_episodes=224, mean=-103.76, std=17.90, min=-124.99, max=-45.82\n",
      "val epoch: train_episodes=256, mean=-115.12, std=23.98, min=-158.54, max=-81.50\n",
      "val epoch: train_episodes=288, mean=-122.21, std=45.49, min=-191.53, max=-2.85\n",
      "val epoch: train_episodes=320, mean=-92.69, std=46.63, min=-206.93, max=-39.18\n",
      "val epoch: train_episodes=352, mean=-73.05, std=43.04, min=-184.11, max=-11.80\n",
      "val epoch: train_episodes=384, mean=-78.53, std=48.93, min=-226.75, max=-9.51\n",
      "val epoch: train_episodes=416, mean=-77.90, std=34.82, min=-175.34, max=-34.50\n",
      "val epoch: train_episodes=448, mean=-78.54, std=29.19, min=-151.02, max=-14.88\n",
      "val epoch: train_episodes=480, mean=-60.46, std=18.14, min=-89.14, max=-19.70\n",
      "val epoch: train_episodes=512, mean=-46.14, std=43.72, min=-160.66, max=21.14\n",
      "val epoch: train_episodes=544, mean=-85.71, std=20.32, min=-140.52, max=-51.58\n",
      "val epoch: train_episodes=576, mean=-84.17, std=30.68, min=-139.53, max=-21.78\n",
      "val epoch: train_episodes=608, mean=-32.26, std=40.19, min=-96.52, max=24.62\n",
      "val epoch: train_episodes=640, mean=-27.34, std=40.20, min=-88.55, max=52.09\n",
      "val epoch: train_episodes=672, mean=-26.93, std=17.84, min=-56.43, max=8.13\n",
      "val epoch: train_episodes=704, mean=4.11, std=26.23, min=-43.58, max=45.97\n",
      "val epoch: train_episodes=736, mean=-52.76, std=59.20, min=-194.74, max=38.80\n",
      "val epoch: train_episodes=768, mean=36.66, std=65.03, min=-43.05, max=213.89\n",
      "val epoch: train_episodes=800, mean=-75.41, std=25.89, min=-128.63, max=-38.72\n",
      "val epoch: train_episodes=832, mean=-56.24, std=34.03, min=-117.13, max=-5.43\n",
      "val epoch: train_episodes=864, mean=-40.12, std=41.59, min=-89.33, max=36.16\n",
      "val epoch: train_episodes=896, mean=-34.54, std=29.53, min=-95.80, max=1.08\n",
      "val epoch: train_episodes=928, mean=0.67, std=41.31, min=-93.45, max=58.26\n",
      "val epoch: train_episodes=960, mean=-7.88, std=30.03, min=-51.14, max=59.21\n",
      "val epoch: train_episodes=992, mean=-7.75, std=38.35, min=-97.34, max=49.44\n",
      "val epoch: train_episodes=1024, mean=-39.30, std=20.82, min=-75.30, max=-6.18\n",
      "val epoch: train_episodes=1056, mean=0.19, std=24.31, min=-46.82, max=45.86\n",
      "val epoch: train_episodes=1088, mean=-11.14, std=56.58, min=-71.11, max=185.47\n",
      "val epoch: train_episodes=1120, mean=-26.38, std=24.95, min=-88.01, max=8.03\n",
      "val epoch: train_episodes=1152, mean=30.84, std=80.95, min=-84.35, max=216.90\n",
      "val epoch: train_episodes=1184, mean=-12.73, std=27.65, min=-64.67, max=54.92\n",
      "val epoch: train_episodes=1216, mean=31.53, std=80.55, min=-50.82, max=253.66\n",
      "val epoch: train_episodes=1248, mean=6.63, std=43.79, min=-46.29, max=124.87\n",
      "val epoch: train_episodes=1280, mean=-13.54, std=90.67, min=-243.49, max=109.57\n",
      "val epoch: train_episodes=1312, mean=40.43, std=106.92, min=-143.25, max=221.67\n",
      "val epoch: train_episodes=1344, mean=-64.17, std=204.23, min=-534.73, max=249.93\n",
      "val epoch: train_episodes=1376, mean=-59.33, std=62.20, min=-145.23, max=60.05\n",
      "val epoch: train_episodes=1408, mean=36.50, std=75.77, min=-50.97, max=248.34\n",
      "val epoch: train_episodes=1440, mean=68.39, std=62.68, min=-12.08, max=205.21\n",
      "val epoch: train_episodes=1472, mean=52.10, std=78.48, min=-122.44, max=160.30\n",
      "val epoch: train_episodes=1504, mean=2.00, std=121.78, min=-248.52, max=146.04\n",
      "val epoch: train_episodes=1536, mean=14.60, std=158.33, min=-277.25, max=269.74\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrsrch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvpg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mvpg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/next/code/rsrch/rsrch/vpg.py:189\u001b[0m, in \u001b[0;36mvpg\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m env\u001b[39m.\u001b[39mreset(seed\u001b[39m=\u001b[39menv_seed)\n\u001b[1;32m    188\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m--> 189\u001b[0m     train_epoch()\n\u001b[1;32m    190\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m val_epoch_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    191\u001b[0m         val_epoch()\n",
      "File \u001b[0;32m~/next/code/rsrch/rsrch/vpg.py:129\u001b[0m, in \u001b[0;36mvpg.<locals>.train_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    123\u001b[0m         episode \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m train_episodes\n\u001b[1;32m    124\u001b[0m         \u001b[39mor\u001b[39;00m ep_step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m max_ep_length\n\u001b[1;32m    125\u001b[0m         \u001b[39mor\u001b[39;00m overall_step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m train_steps\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m stop():\n\u001b[0;32m--> 129\u001b[0m     action \u001b[39m=\u001b[39m ac\u001b[39m.\u001b[39;49mact(to_tensor(obs, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32))\n\u001b[1;32m    130\u001b[0m     next_obs, reward, term, trunc, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m    131\u001b[0m     ep_observations\u001b[39m.\u001b[39mappend(obs)\n",
      "File \u001b[0;32m~/next/code/rsrch/rsrch/vpg.py:87\u001b[0m, in \u001b[0;36mActorCritic.act\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mact\u001b[39m(\u001b[39mself\u001b[39m, obs: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m---> 87\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpi\u001b[39m.\u001b[39;49mact(obs)\n",
      "File \u001b[0;32m~/next/code/rsrch/rsrch/vpg.py:50\u001b[0m, in \u001b[0;36mPolicy.act\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mact\u001b[39m(\u001b[39mself\u001b[39m, obs: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[39mwith\u001b[39;00m eval_ctx(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 50\u001b[0m         pi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(obs\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n\u001b[1;32m     51\u001b[0m         \u001b[39mreturn\u001b[39;00m pi\u001b[39m.\u001b[39msample()[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/mambaforge/envs/rsrch/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/next/code/rsrch/rsrch/vpg.py:36\u001b[0m, in \u001b[0;36mPolicy.forward\u001b[0;34m(self, obs, act)\u001b[0m\n\u001b[1;32m     34\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(obs)\n\u001b[1;32m     35\u001b[0m obs \u001b[39m=\u001b[39m obs\u001b[39m.\u001b[39mreshape(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m log_pi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(obs)\n\u001b[1;32m     37\u001b[0m pi \u001b[39m=\u001b[39m Categorical(logits\u001b[39m=\u001b[39mlog_pi)\n\u001b[1;32m     39\u001b[0m \u001b[39mif\u001b[39;00m act \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/rsrch/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/rsrch/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/rsrch/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/rsrch/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from rsrch.vpg import *\n",
    "vpg()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('rsrch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b948e45c948dd176e3efe7e1cfd0e5e7237983d8db14ea5811853abda2d845c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
