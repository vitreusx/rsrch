{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3375891183.py, line 188)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 188\u001b[0;36m\u001b[0m\n\u001b[0;31m    def repr_model(self, h: RSSMState, obs: Tensor, act: Tensor) -> RSSM\u001b[0m\n\u001b[0m                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "from typing import List, NamedTuple\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dims: List[int]):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        for idx, (d_in, d_out) in enumerate(zip(dims[:-1], dims[1:])):\n",
    "            if idx > 0:\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(d_in, d_out, bias=True))\n",
    "        \n",
    "        self.main = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.main(x)\n",
    "\n",
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self, obs_shape: torch.Size, kernel_size=3, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.obs_shape = obs_shape\n",
    "        self.kernel_size = k = kernel_size\n",
    "        self.hidden_dim = h = hidden_dim\n",
    "\n",
    "        c, W, H = self.obs_shape\n",
    "        assert W % 16 == 0 and H % 16 == 0, \\\n",
    "            \"image resolution should be divisible by 16\"\n",
    "        assert k % 2 == 1, \\\n",
    "            \"kernel_size should be an odd number\"\n",
    "        p = k // 2\n",
    "\n",
    "        final_size = torch.Size([H // 16, W // 16, 8*h])\n",
    "        self.out_features = final_size.numel()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(c, h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(h, 2*h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(2*h, 4*h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(4*h, 8*h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.main(x)\n",
    "\n",
    "class NormalLinear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.out_features = out_features\n",
    "        self.fc = nn.Linear(in_features, 2*out_features)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> dist.Distribution:\n",
    "        params = self.fc(x)\n",
    "        mean, std = params.split(self.out_features)\n",
    "        res_dist = dist.Normal(mean, std)\n",
    "        res_dist = dist.Independent(res_dist, 1)\n",
    "        return res_dist\n",
    "\n",
    "class VisualDecoder(nn.Module):\n",
    "    def __init__(self, in_features: int, obs_shape: torch.Size, kernel_size=3, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.obs_shape = obs_shape\n",
    "        self.kernel_size = k = kernel_size\n",
    "        self.hidden_dim = h = hidden_dim\n",
    "\n",
    "        c, self.W, self.H = self.obs_shape\n",
    "        p = k // 2\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(8*h, 4*h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(4*h, 2*h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(2*h, h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(h, c, k, 1, p),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: Tensor) -> dist.Distribution:\n",
    "        x = x.reshape(len(x), -1, self.W // 16, self.H // 16)\n",
    "        obs_dist = dist.Normal(self.main(x), 1.0)\n",
    "        # By default, we'd get a single normal distribution over [B, C, H, W], \n",
    "        # whereas we actually want B independent normal distributions over\n",
    "        # [C, H, W] each. This is what Independent is for.\n",
    "        obs_dist = dist.Independent(obs_dist, 3) \n",
    "        return obs_dist\n",
    "\n",
    "class RSSMState(NamedTuple):\n",
    "    deter: Tensor\n",
    "    stoch: Tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return RSSMState(self.deter[idx], self.stoch[idx])\n",
    "    \n",
    "    def __setitem__(self, idx, value: RSSMState):\n",
    "        self.deter[idx] = value.deter\n",
    "        self.stoch[idx] = value.stoch\n",
    "\n",
    "    @staticmethod\n",
    "    def zeros(*size: int):\n",
    "        return RSSMState(deter=torch.zeros(*size), stoch=torch.zeros(*size))\n",
    "\n",
    "    def clone(self):\n",
    "        return RSSMState(self.deter.clone(), self.stoch.clone())\n",
    "\n",
    "class RSSMStateDist(NamedTuple):\n",
    "    deter: Tensor\n",
    "    stoch_dist: dist.Distribution\n",
    "\n",
    "    def sample_n(self, n: int) -> RSSMState:\n",
    "        return self.sample((n,))\n",
    "\n",
    "    def sample(self, sample_size: torch.Size = torch.Size()) -> RSSMState:\n",
    "        deter = self.deter.expand(*sample_size, *self.deter.shape)\n",
    "        stoch = self.stoch_dist.sample(sample_size)\n",
    "        return RSSMState(deter, stoch)\n",
    "    \n",
    "    def rsample(self, sample_size: torch.Size = torch.Size()) -> RSSMState:\n",
    "        deter = self.deter.expand(*sample_size, *self.deter.shape)\n",
    "        stoch = self.stoch_dist.rsample(sample_size)\n",
    "        return RSSMState(deter, stoch)\n",
    "    \n",
    "    def log_prob(self, state: RSSMState):\n",
    "        return self.stoch_dist.log_prob(state.stoch)\n",
    "\n",
    "    def detach(self) -> RSSMStateDist:\n",
    "        deter = self.deter.detach()\n",
    "        if isinstance(self.stoch_dist, dist.Normal):\n",
    "            stoch_loc = self.stoch_dist.loc.detach()\n",
    "            stoch_scale = self.stoch_dist.scale.detach()\n",
    "            stoch_dist = dist.Normal(stoch_loc, stoch_scale)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Cannot detach {type(self.stoch_dist)}\")\n",
    "\n",
    "        return RSSMStateDist(deter, stoch_dist)\n",
    "\n",
    "@dist.register_kl(RSSMStateDist, RSSMStateDist)\n",
    "def kl_divergence(p: RSSMStateDist, q: RSSMStateDist):\n",
    "    return dist.kl_divergence(p.stoch_dist, q.stoch_dist)\n",
    "\n",
    "class EnvStateBatch(NamedTuple):\n",
    "    obs: Tensor\n",
    "    act: Tensor\n",
    "    reward: Tensor\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.obs.shape\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from zip(self.obs, self.act, self.reward)\n",
    "\n",
    "class RSSMCell(nn.Module):\n",
    "    def __init__(self, in_features: int, deter_dim: int, stoch_dim: int):\n",
    "        super().__init__()\n",
    "        self.deter_dim = deter_dim\n",
    "        self.stoch_dim = stoch_dim\n",
    "\n",
    "        deter_input_size = deter_dim + stoch_dim + in_features\n",
    "        self.deter_state_model = nn.GRUCell(deter_input_size, deter_dim)\n",
    "        self.stoch_state_model = nn.Sequential(\n",
    "            MLP([deter_dim, 256]), \n",
    "            nn.ReLU(),\n",
    "            NormalLinear(256, stoch_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, h: RSSMState, x: Tensor) -> RSSMStateDist:\n",
    "        deter_x = torch.cat([h.deter, h.stoch, x], dim=1)\n",
    "        deter: Tensor = self.deter_state_model(deter_x)\n",
    "        stoch_dist = self.stoch_state_model(deter)\n",
    "        return RSSMStateDist(deter, stoch_dist)\n",
    "\n",
    "\n",
    "class PlaNetRSSM(nn.Module):\n",
    "    def __init__(self, obs_shape: torch.Size, act_shape: torch.Size, deter_dim: int, stoch_dim: int):\n",
    "        super().__init__()\n",
    "        self.deter_dim = deter_dim\n",
    "        self.stoch_dim = stoch_dim\n",
    "        \n",
    "        self.vis_encoder = VisualEncoder(obs_shape)\n",
    "        repr_model_input = self.vis_encoder.out_features + act_shape.numel()\n",
    "        self.repr_cell = RSSMCell(repr_model_input, deter_dim, stoch_dim)\n",
    "        trans_model_input = act_shape.numel()\n",
    "        self.trans_cell = RSSMCell(trans_model_input, deter_dim, stoch_dim)\n",
    "        self.vis_decoder = VisualDecoder(deter_dim + stoch_dim, obs_shape)\n",
    "        self.reward_net = nn.Sequential(\n",
    "            MLP([deter_dim + stoch_dim, 256]),\n",
    "            nn.ReLU(),\n",
    "            NormalLinear(256, 1),\n",
    "        )\n",
    "    \n",
    "    def repr_model(self, h: RSSMState, obs: Tensor, act: Tensor) -> RSSMStateDist:\n",
    "        obs_z = self.vis_encoder(obs)\n",
    "        act_z = act.reshape(len(act), -1)\n",
    "        repr_x = torch.cat([obs_z, act_z], dim=1)\n",
    "        return self.repr_cell(h, repr_x)\n",
    "\n",
    "    def trans_model(self, h: RSSMState, act: Tensor) -> RSSMStateDist:\n",
    "        act_z = act.reshape(len(act), -1)\n",
    "        trans_x = act_z\n",
    "        return self.trans_cell(h, trans_x)\n",
    "\n",
    "    def obs_model(self, h: RSSMState) -> dist.Distribution:\n",
    "        state = torch.stack([h.deter, h.stoch], dim=1)\n",
    "        return self.vis_decoder(state)\n",
    "\n",
    "    def reward_model(self, h: RSSMState) -> dist.Distribution:\n",
    "        state = torch.cat([h.deter, h.stoch], 1)\n",
    "        return self.reward_net(state)\n",
    "\n",
    "    def loss(self, batch: EnvStateBatch):\n",
    "        seq_len, batch_size = batch.shape[:2]\n",
    "\n",
    "        cur_state = RSSMState.zeros(batch_size)\n",
    "        repr_dists: List[RSSMStateDist | None] = [None]\n",
    "        states: List[RSSMState] = [cur_state]\n",
    "\n",
    "        # Predicting states with observations + Reconstruction loss\n",
    "        recon_loss = 0.0\n",
    "        for obs, act, reward in batch:\n",
    "            # Compute the informed state distribution\n",
    "            repr_dist: RSSMStateDist = self.repr_model(cur_state, obs, act)\n",
    "\n",
    "            # NOTE: Not sure whether to detach prior or not\n",
    "            # Dreamer v2, in \"KL balancing\" section, mixes both\n",
    "            # In PlaNet paper, it is suggested in \"Latent overshooting\"\n",
    "            # section that the repr_dist is detached for overshooting dist > 1\n",
    "            # whereas here we stop it altogether from the get-go.\n",
    "            repr_dists.append(repr_dist.detach())\n",
    "            # repr_dists.append(repr_dist)\n",
    "\n",
    "            # Sample a state from the distribution\n",
    "            # NOTE: sample() or rsample() ?\n",
    "            cur_state = repr_dist.rsample()\n",
    "            states.append(cur_state)\n",
    "\n",
    "            # Evaluate observation and reward recon losses\n",
    "            obs_dist = self.obs_model(cur_state)\n",
    "            obs_loss = -obs_dist.log_prob(obs)\n",
    "            reward_dist = self.reward_model(cur_state)\n",
    "            reward_loss = -reward_dist.log_prob(reward)\n",
    "            recon_loss += obs_loss + reward_loss\n",
    "        \n",
    "        # Overshooting\n",
    "        trans_states = [x.clone() for x in states]\n",
    "        trans_loss = 0.0\n",
    "        for steps_ahead in range(1, seq_len):\n",
    "            next_trans_states = []\n",
    "            for step_idx in range(steps_ahead-1, seq_len):\n",
    "                # Compute transition-model next state distribution\n",
    "                trans_h: RSSMState = trans_states[step_idx]\n",
    "                act = batch.act[step_idx]\n",
    "                trans_dist: RSSMStateDist = self.trans_model(trans_h, act)\n",
    "\n",
    "                # Sample a state from said distribution\n",
    "                trans_state = trans_dist.rsample()\n",
    "                next_trans_states.append(trans_state)\n",
    "\n",
    "                # \"Latent overshooting\"\n",
    "                repr_dist: RSSMStateDist = repr_dists[step_idx+1] # type: ignore\n",
    "                trans_loss += dist.kl_divergence(trans_dist, repr_dist)\n",
    "\n",
    "                # # \"Observation overshooting\"\n",
    "                # obs_dist = self.obs_model(trans_state)\n",
    "                # obs_loss = -obs_dist.log_prob(batch.obs[step_idx])\n",
    "                # reward_dist = self.reward_model(trans_state)\n",
    "                # reward_loss = -reward_dist.log_prob(batch.reward[step_idx])\n",
    "                # trans_loss += obs_loss + reward_loss\n",
    "\n",
    "            trans_states[steps_ahead:] = next_trans_states\n",
    "        \n",
    "        # # \"Standard variational bound\"\n",
    "        # trans_loss = 0.0\n",
    "        # for step_idx in range(seq_len):\n",
    "        #     trans_h = states[step_idx]\n",
    "        #     trans_x = batch.act[step_idx].reshape(batch_size, -1)\n",
    "        #     trans_dist: RSSMStateDist = self.trans_model(trans_h, trans_x)\n",
    "        #     repr_dist: RSSMStateDist = repr_dists[step_idx+1] # type: ignore\n",
    "        #     trans_loss += dist.kl_divergence(trans_dist, repr_dist)\n",
    "\n",
    "        total_loss = recon_loss + trans_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEMPlanner:\n",
    "    def __init__(self, horizon: int, optim_iters: int, iter_pop: int, iter_top_k: int, act_shape: torch.Size):\n",
    "        self.horizon = self.H = horizon\n",
    "        self.optim_iters = self.I = optim_iters\n",
    "        self.iter_pop = self.J = iter_pop\n",
    "        self.iter_top_k = self.K = iter_top_k\n",
    "        self.act_shape = act_shape\n",
    "    \n",
    "    def next_action(self, cur_state_dist: RSSMStateDist, rssm: PlaNetRSSM):\n",
    "        # Initialize action sequence distribution\n",
    "        mean = torch.zeros((self.H, *self.act_shape))\n",
    "        std = torch.ones((self.H, *self.act_shape))\n",
    "        action_seq_dist = dist.Normal(mean, std)\n",
    "\n",
    "        rewards = torch.zeros(self.J, requires_grad=False)\n",
    "\n",
    "        for iter in range(self.I):\n",
    "            # Sample actions from the distribution\n",
    "            actions = action_seq_dist.sample_n(self.J)\n",
    "\n",
    "            # Execute actions and predict the rewards\n",
    "            rewards.zero_()\n",
    "            state = cur_state_dist.sample_n(self.J)\n",
    "            rewards += rssm.reward_model(state).mean\n",
    "            for step_idx in range(self.H):\n",
    "                state = rssm.trans_model(state, actions[:,step_idx]).sample()\n",
    "                rewards += rssm.reward_model(state).mean\n",
    "            \n",
    "            # Select the best sequences and update the distribution\n",
    "            best_idxes = torch.argmax(rewards)[:self.K]\n",
    "            best_actions = actions[best_idxes]\n",
    "            mean, std = best_actions.mean(0), best_actions.var(0)\n",
    "            action_seq_dist = dist.Normal(mean, std)\n",
    "        \n",
    "        # Return mean first action value\n",
    "        return mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generic, TypeVar, NamedTuple\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "class stack(Generic[T]):\n",
    "    def __init__(self):\n",
    "        self._data: list[T] = []\n",
    "        self._start = 0\n",
    "    \n",
    "    def push(self, x: T):\n",
    "        self._data.append(x)\n",
    "        if self._start >= len(self._data):\n",
    "            self._data = self._data[self._start:]\n",
    "            self._start = 0\n",
    "    \n",
    "    def top(self) -> T:\n",
    "        return self._data[-1]\n",
    "    \n",
    "    def pop(self) -> T:\n",
    "        item = self._data[self._start]\n",
    "        self._start += 1\n",
    "        return item\n",
    "\n",
    "class TransitionBatch(NamedTuple):\n",
    "    obs: Tensor\n",
    "    act: Tensor\n",
    "    next_obs: Tensor\n",
    "    reward: Tensor\n",
    "    done: Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size: torch.Size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        C, H, W = input_size[-3:]\n",
    "        self.kernel_size = k = 3\n",
    "        self.hidden_dim = h = 64\n",
    "        self.latent_dim = z_dim = 64\n",
    "        p = k // 2\n",
    "\n",
    "        conv_size = torch.Size((8*h, H//16, W//16))\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(C, h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(h, 2*h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(2*h, 4*h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(4*h, 8*h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            MLP([conv_size.numel(), 256, z_dim]),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: Tensor) -> dist.Distribution:\n",
    "        params = self.main(x)\n",
    "        mean, std = torch.split(params, self.latent_dim)\n",
    "        z_dist = dist.Normal(mean, std)\n",
    "        z_dist = dist.Independent(z_dist, 1)\n",
    "        return z_dist\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size: torch.Size):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        C, H, W = output_size\n",
    "        self.hidden_dim = h = 64\n",
    "        self.latent_dim = z_dim = 64\n",
    "        self.kernel_size = k = 3\n",
    "        self.conv_size = torch.Size((8*h, H // 16, W // 16))\n",
    "\n",
    "        p = k // 2\n",
    "\n",
    "        self.z_map = MLP([z_dim, 64, self.conv_size.numel()])\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(8*h, 4*h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(4*h, 2*h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(2*h, h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(h, C, k, 1, p),\n",
    "        )\n",
    "    \n",
    "    def forward(self, z: Tensor) -> dist.Distribution:\n",
    "        mean = self.main(self.z_map(z))\n",
    "        x_dist = dist.Normal(mean, 1.0)\n",
    "        x_dist = dist.Independent(x_dist, 3)\n",
    "        return x_dist\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size: torch.Size):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(input_size)\n",
    "        self.dec = Decoder(input_size)\n",
    "        \n",
    "        self.z_prior = dist.Normal(\n",
    "            loc=torch.zeros(self.enc.latent_dim),\n",
    "            scale=torch.ones(self.enc.latent_dim),\n",
    "        )\n",
    "    \n",
    "    def sample(self, sample_shape: torch.Size = torch.Size()) -> Tensor:\n",
    "        zs = self.z_prior.sample(sample_shape)\n",
    "        x_dists: dist.Distribution = self.dec(zs)\n",
    "        return x_dists.sample()\n",
    "    \n",
    "    def loss(self, x: Tensor) -> Tensor:\n",
    "        z_dist: dist.Distribution = self.enc(x)\n",
    "        z = z_dist.rsample()\n",
    "        x_hat: dist.Distribution = self.dec(z)\n",
    "        prior_loss = dist.kl_divergence(self.z_prior, z_dist)\n",
    "        recon_loss = x_hat.log_prob(x)\n",
    "        total_loss = prior_loss + recon_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Space, Discrete, Box\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, num_actions: int):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_shape: torch.Size):\n",
    "        super().__init__()\n",
    "        self.vis_enc = VisualEncoder(obs_shape)\n",
    "        self.fc = MLP([self.vis_enc.out_features, 256, 1])\n",
    "    \n",
    "    def forward(self):\n",
    "        ...\n",
    "\n",
    "class ObsEncoder(nn.Module):\n",
    "    def __init__(self, obs_space: Space, enc_dim: int):\n",
    "        super().__init__()\n",
    "        if isinstance(obs_space, Box):\n",
    "            obs_shape = torch.Size(obs_space.shape)\n",
    "            self.enc = VisualEncoder(obs_shape)\n",
    "            self.enc = nn.Sequential(self.enc, nn.Linear(self.enc.out_features, enc_dim))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def forward(self, obs: Tensor) -> Tensor:\n",
    "        return self.enc(obs)\n",
    "\n",
    "class ActionEncoder(nn.Module):\n",
    "    def __init__(self, act_space: Space, enc_dim: int):\n",
    "        super().__init__()\n",
    "        if isinstance(act_space, Discrete):\n",
    "            self.enc = nn.Embedding(act_space.n, enc_dim)\n",
    "        elif isinstance(act_space, Box):\n",
    "            shape = torch.Size(act_space.shape)\n",
    "            self.enc = MLP([shape.numel(), 256, enc_dim])\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    \n",
    "    def forward(self, act: Tensor) -> Tensor:\n",
    "        return self.enc(act)\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_shape: torch.Size, num_actions: int):\n",
    "        super().__init__()\n",
    "        self.obs_enc = VisualEncoder(obs_shape)\n",
    "        self.actor = MLP([self.obs_enc.out_features, 256, num_actions])\n",
    "        self.critic = MLP([self.obs_enc.out_features + num_actions])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
