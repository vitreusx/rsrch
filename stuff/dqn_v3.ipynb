{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import NamedTuple, List, Protocol, Any, Sequence, Optional, Protocol\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium.wrappers\n",
    "from rsrch.utils.polyak import Polyak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Sequential):\n",
    "    def __init__(self, num_features: List[int], norm_layer=nn.LayerNorm, act_layer=nn.ReLU):\n",
    "        layers = []\n",
    "        seq = enumerate(zip(num_features[:-1], num_features[1:]))\n",
    "        final_layer_idx = len(num_features) - 2\n",
    "        for layer_idx, (in_features, out_features) in seq:\n",
    "            if layer_idx > 0:\n",
    "                layers.append(norm_layer(in_features))\n",
    "                layers.append(act_layer())\n",
    "            bias = (layer_idx == final_layer_idx)\n",
    "            layers.append(nn.Linear(in_features, out_features, bias=bias))\n",
    "        \n",
    "        super().__init__(*layers)\n",
    "\n",
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self, obs_shape: torch.Size, kernel_size=3, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.obs_shape = obs_shape\n",
    "        self.kernel_size = k = kernel_size\n",
    "        self.hidden_dim = h = hidden_dim\n",
    "\n",
    "        c, W, H = self.obs_shape\n",
    "        assert W % 16 == 0 and H % 16 == 0, \\\n",
    "            \"image resolution should be divisible by 16\"\n",
    "        assert k % 2 == 1, \\\n",
    "            \"kernel_size should be an odd number\"\n",
    "        p = k // 2\n",
    "\n",
    "        final_size = torch.Size([H // 16, W // 16, 8*h])\n",
    "        self.out_features = final_size.numel()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(c, h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(h, 2*h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(2*h, 4*h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(4*h, 8*h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.main(x)\n",
    "\n",
    "class ObsEncoder(nn.Module):\n",
    "    def __init__(self, obs_space: gym.Space, enc_dim: int):\n",
    "        super().__init__()\n",
    "        self.out_features = enc_dim\n",
    "        \n",
    "        if isinstance(obs_space, gym.spaces.Box):\n",
    "            obs_shape = torch.Size(obs_space.shape)\n",
    "            if len(obs_shape) >= 3:\n",
    "                self.enc = VisualEncoder(obs_shape)\n",
    "                self.enc = nn.Sequential(self.enc, nn.Linear(self.enc.out_features, enc_dim))\n",
    "            else:\n",
    "                self.enc = MLP([obs_shape.numel(), 256, enc_dim])\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def forward(self, obs: Tensor) -> Tensor:\n",
    "        return self.enc(obs)\n",
    "\n",
    "class ActionEncoder(nn.Module):\n",
    "    def __init__(self, act_space: gym.Space, enc_dim: int):\n",
    "        super().__init__()\n",
    "        self.out_features = enc_dim\n",
    "\n",
    "        if isinstance(act_space, gym.spaces.Discrete):\n",
    "            self.enc = nn.Embedding(act_space.n, enc_dim)\n",
    "        elif isinstance(act_space, gym.spaces.Box):\n",
    "            shape = torch.Size(act_space.shape)\n",
    "            self.enc = MLP([shape.numel(), 256, enc_dim])\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    \n",
    "    def forward(self, act: Tensor) -> Tensor:\n",
    "        return self.enc(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventHandler(Protocol):\n",
    "    def on_reset(self, obs):\n",
    "        ...\n",
    "    \n",
    "    def on_step(self, act, next_obs, reward, term, trunc):\n",
    "        ...\n",
    "\n",
    "class InvokeHandler(gym.Wrapper):\n",
    "    def __init__(self, env: gym.Env, handler: EventHandler):\n",
    "        super().__init__(env)\n",
    "        self._handler = handler\n",
    "    \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        obs, info = self.env.reset(seed=seed, options=options)\n",
    "        self._handler.on_reset(obs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, act):\n",
    "        next_obs, reward, term, trunc, info = self.env.step(act)\n",
    "        self._handler.on_step(act, next_obs, reward, term, trunc)\n",
    "        return next_obs, reward, term, trunc, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(NamedTuple):\n",
    "    obs: Tensor\n",
    "    act: Tensor\n",
    "    next_obs: Tensor\n",
    "    reward: Tensor\n",
    "    term: Tensor\n",
    "    trunc: Tensor\n",
    "    \n",
    "    def to(self, device: torch.device):\n",
    "        return Transition(\n",
    "            obs=self.obs.to(device),\n",
    "            act=self.act.to(device),\n",
    "            next_obs=self.next_obs.to(device),\n",
    "            reward=self.reward.to(device),\n",
    "            term=self.term.to(device),\n",
    "            trunc=self.trunc.to(device),\n",
    "        )\n",
    "\n",
    "class TransitionBuffer(data.Dataset):\n",
    "    def __init__(self, env: gym.Env, capacity: int):\n",
    "        def buffer_for(space=None, dtype=None):\n",
    "            if space is not None:\n",
    "                x = np.empty(space.shape, dtype=space.dtype)\n",
    "                x = torch.from_numpy(x)\n",
    "                x = torch.empty(capacity, *x.shape, dtype=x.dtype)\n",
    "            else:\n",
    "                x = torch.empty(capacity, dtype=dtype)\n",
    "            return x\n",
    "\n",
    "        self.obs = buffer_for(space=env.observation_space)\n",
    "        self.act = buffer_for(space=env.action_space)\n",
    "        self.next_obs = torch.empty_like(self.obs)\n",
    "        self.reward = buffer_for(dtype=torch.float)\n",
    "        self.term = buffer_for(dtype=torch.bool)\n",
    "        self.trunc = buffer_for(dtype=torch.bool)\n",
    "\n",
    "        self._capacity = capacity\n",
    "        self._cursor = self._size = 0\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.obs.device\n",
    "    \n",
    "    def _convert(self, x, type_as):\n",
    "        return torch.as_tensor(x).type_as(type_as)\n",
    "    \n",
    "    def push(self, obs, act, next_obs, reward, term, trunc):\n",
    "        idx = self._cursor\n",
    "        self.obs[idx] = self._convert(obs, self.obs)\n",
    "        self.act[idx] = self._convert(act, self.act)\n",
    "        self.next_obs[idx] = self._convert(next_obs, self.next_obs)\n",
    "        self.reward[idx] = reward\n",
    "        self.term[idx] = term\n",
    "        self.trunc[idx] = trunc\n",
    "\n",
    "        self._cursor = (self._cursor + 1) % self._capacity\n",
    "        if self._size < self._capacity:\n",
    "            self._size += 1\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return Transition(\n",
    "            obs=self.obs[idx],\n",
    "            act=self.act[idx],\n",
    "            next_obs=self.next_obs[idx],\n",
    "            reward=self.reward[idx],\n",
    "            term=self.term[idx],\n",
    "            trunc=self.trunc[idx],\n",
    "        )\n",
    "\n",
    "class CollectTransitions(gym.Wrapper):\n",
    "    def __init__(self, env: gym.Env, buffer: TransitionBuffer):\n",
    "        super().__init__(env)\n",
    "        self._buffer = buffer\n",
    "    \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        obs, info = self.env.reset(seed=seed, options=options)\n",
    "        self._obs = obs\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, act):\n",
    "        next_obs, reward, term, trunc, info = self.env.step(act)\n",
    "        self._buffer.push(self._obs, act, next_obs, reward, term, trunc)\n",
    "        self._obs = next_obs\n",
    "        return next_obs, reward, term, trunc, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trajectory(Protocol):\n",
    "    obs: Sequence\n",
    "    act: Sequence\n",
    "    reward: Sequence\n",
    "    trunc: Sequence\n",
    "    term: Sequence\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        ...\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ...\n",
    "\n",
    "class TensorTrajectory(NamedTuple):\n",
    "    obs: Tensor\n",
    "    act: Tensor\n",
    "    reward: Tensor\n",
    "    trunc: Tensor\n",
    "    term: Tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def as_tensor(tr: Trajectory):\n",
    "        return TensorTrajectory(\n",
    "            obs=torch.tensor(tr.obs),\n",
    "            act=torch.tensor(tr.act),\n",
    "            reward=torch.tensor(tr.reward),\n",
    "            trunc=torch.tensor(tr.trunc),\n",
    "            term=torch.tensor(tr.term),\n",
    "        )\n",
    "\n",
    "    def clone(self):\n",
    "        return TensorTrajectory(\n",
    "            obs=self.obs.clone(),\n",
    "            act=self.act.clone(),\n",
    "            reward=self.reward.clone(),\n",
    "            trunc=self.trunc.clone(),\n",
    "            term=self.term.clone(),\n",
    "        )\n",
    "\n",
    "    def to(self, device: torch.device):\n",
    "        return TensorTrajectory(\n",
    "            obs=self.obs.to(device),\n",
    "            act=self.act.to(device),\n",
    "            reward=self.reward.to(device),\n",
    "            trunc=self.trunc.to(device),\n",
    "            term=self.term.to(device),\n",
    "        )\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return TensorTrajectory(\n",
    "            obs=self.obs[idx],\n",
    "            act=self.act[idx],\n",
    "            reward=self.reward[idx],\n",
    "            trunc=self.trunc[idx],\n",
    "            term=self.term[idx],\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.obs)\n",
    "\n",
    "class MmapTrajectory(NamedTuple):\n",
    "    obs: np.memmap\n",
    "    act: np.memmap\n",
    "    reward: np.memmap\n",
    "    trunc: np.memmap\n",
    "    term: np.memmap\n",
    "    \n",
    "    @staticmethod\n",
    "    def open(root: Path):\n",
    "        return MmapTrajectory(\n",
    "            obs=np.load(root / \"obs.npy\", mmap_mode=\"r\"),\n",
    "            act=np.load(root / \"act.npy\", mmap_mode=\"r\"),\n",
    "            reward=np.load(root / \"reward.npy\", mmap_mode=\"r\"),\n",
    "            trunc=np.load(root / \"trunc.npy\", mmap_mode=\"r\"),\n",
    "            term=np.load(root / \"term.npy\", mmap_mode=\"r\"),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def save(tr: Trajectory, root: Path):\n",
    "        root.mkdir(parents=True, exist_ok=True)\n",
    "        np.save(root / \"obs.npy\", np.asarray(tr.obs))\n",
    "        np.save(root / \"act.npy\", np.asarray(tr.act))\n",
    "        np.save(root / \"reward.npy\", np.asarray(tr.reward))\n",
    "        np.save(root / \"trunc.npy\", np.asarray(tr.trunc))\n",
    "        np.save(root / \"term.npy\", np.asarray(tr.term))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.obs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return MmapTrajectory(obs=self.obs[idx], act=self.act[idx],\n",
    "                              reward=self.reward[idx], trunc=self.trunc[idx],\n",
    "                              term=self.term[idx])\n",
    "\n",
    "class Subsample(nn.Module):\n",
    "    def __init__(self, seq_len: int):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def forward(self, traj: Trajectory):\n",
    "        start = np.random.randint(len(traj))\n",
    "        end = start + self.seq_len\n",
    "        return traj[start:end]\n",
    "\n",
    "class ToTensor(nn.Module):\n",
    "    def forward(self, traj: Trajectory):\n",
    "        return TensorTrajectory.as_tensor(traj)\n",
    "\n",
    "class MapDs(data.Dataset):\n",
    "    def __init__(self, ds: data.Dataset, f):\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.f = f\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.f(self.ds[idx])\n",
    "\n",
    "class MapIterDs(data.IterableDataset):\n",
    "    def __init__(self, ds: data.IterableDataset, f):\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.f = f\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for x in self.ds:\n",
    "            yield self.f(x)\n",
    "\n",
    "class TrajectoryBatch(NamedTuple):\n",
    "    obs: rnn.PackedSequence\n",
    "    act: rnn.PackedSequence\n",
    "    reward: rnn.PackedSequence\n",
    "    trunc: rnn.PackedSequence\n",
    "    term: rnn.PackedSequence\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch: List[TensorTrajectory]) -> TrajectoryBatch:\n",
    "        lengths = torch.as_tensor([len(tr) for tr in batch])\n",
    "        idxes = torch.argsort(lengths, descending=True)\n",
    "        \n",
    "        return TrajectoryBatch(\n",
    "            obs=rnn.pack_sequence([batch[idx].obs for idx in idxes]),\n",
    "            act=rnn.pack_sequence([batch[idx].act for idx in idxes]),\n",
    "            reward=rnn.pack_sequence([batch[idx].reward for idx in idxes]),\n",
    "            trunc=rnn.pack_sequence([batch[idx].trunc for idx in idxes]),\n",
    "            term=rnn.pack_sequence([batch[idx].term for idx in idxes]),\n",
    "        )\n",
    "\n",
    "class EpisodeBuffer(data.Dataset):\n",
    "    def __init__(self, env: gym.Env, max_seq_len: int, seq_capacity: int, \n",
    "                 mmap_root: Optional[Path] = None, device=None):\n",
    "        def buffer_for(space=None, dtype=None):\n",
    "            if space is not None:\n",
    "                x = np.empty(space.shape, dtype=space.dtype)\n",
    "                x = torch.from_numpy(x)\n",
    "                x = torch.empty(max_seq_len, *x.shape, dtype=x.dtype,\n",
    "                                device=device)\n",
    "            else:\n",
    "                x = torch.empty(max_seq_len, dtype=dtype, device=device)\n",
    "            return x\n",
    "\n",
    "        self.obs = buffer_for(space=env.observation_space)\n",
    "        self.act = buffer_for(space=env.action_space)\n",
    "        self.reward = buffer_for(dtype=torch.float)\n",
    "        self.trunc = buffer_for(dtype=torch.bool)\n",
    "        self.trunc.fill_(True)\n",
    "        self.term = buffer_for(dtype=torch.bool)\n",
    "        \n",
    "        self._episodes = np.empty((seq_capacity,), dtype=object)\n",
    "        self.mmap_root = mmap_root\n",
    "\n",
    "        self._cur_step = 0\n",
    "        self._cur_ep_idx = -1\n",
    "        self.num_episodes = 0\n",
    "        self.seq_capacity = seq_capacity\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.obs.device\n",
    "    \n",
    "    def _conv(self, x, type_as):\n",
    "        return torch.as_tensor(x).type_as(type_as)\n",
    "\n",
    "    def on_reset(self, obs):\n",
    "        self._cur_step = self._cur_ep_len = 0\n",
    "        self._cur_ep_idx = (self._cur_ep_idx + 1) % self.seq_capacity\n",
    "        self.num_episodes = min(self.num_episodes + 1, self.seq_capacity)\n",
    "\n",
    "        self.obs[self._cur_step] = self._conv(obs, self.obs)\n",
    "        self.trunc[self._cur_step] = True\n",
    "        self.term[self._cur_step] = False\n",
    "\n",
    "        self._update_cur_episode_view()\n",
    "    \n",
    "    def _update_cur_episode_view(self):\n",
    "        ep_len = self._cur_step + 1\n",
    "        \n",
    "        # NOTE: This only creates a view, no data is copied\n",
    "        self._episodes[self._cur_ep_idx] = TensorTrajectory(\n",
    "            obs=self.obs[:ep_len],\n",
    "            act=self.act[:ep_len],\n",
    "            reward=self.reward[:ep_len],\n",
    "            trunc=self.trunc[:ep_len],\n",
    "            term=self.term[:ep_len],\n",
    "        )\n",
    "    \n",
    "    def on_step(self, act, next_obs, reward, term, trunc):\n",
    "        if self._cur_step < self.max_seq_len:\n",
    "            self.act[self._cur_step] = self._conv(act, self.act)\n",
    "            self.trunc[self._cur_step] = False\n",
    "            self._cur_step += 1\n",
    "            self.obs[self._cur_step] = self._conv(next_obs, self.obs)\n",
    "            self.reward[self._cur_step] = reward\n",
    "            self.term[self._cur_step] = term\n",
    "            self.trunc[self._cur_step] = trunc\n",
    "\n",
    "        self._update_cur_episode_view()\n",
    "\n",
    "        done = term or trunc\n",
    "        if done:\n",
    "            cur_ep_view = self._episodes[self._cur_ep_idx]\n",
    "            if self.mmap_root is not None:\n",
    "                dst_root = self.mmap_root / f\"{self._cur_ep_idx:06d}\"\n",
    "                MmapTrajectory.save(cur_ep_view, dst_root)\n",
    "                self._episodes[self._cur_ep_idx] = MmapTrajectory.open(dst_root)\n",
    "            else:\n",
    "                self._episodes[self._cur_ep_idx] = cur_ep_view.clone()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_episodes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= self.num_episodes:\n",
    "            raise IndexError()\n",
    "        return self._episodes[idx]\n",
    "\n",
    "\n",
    "class CollectEpisodes(gym.Wrapper):\n",
    "    def __init__(self, env: gym.Env, buffer: EpisodeBuffer):\n",
    "        super().__init__(env)\n",
    "        self._buffer = buffer\n",
    "    \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        obs, info = self.env.reset(seed=seed, options=options)\n",
    "        self._buffer.on_reset(obs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, act):\n",
    "        next_obs, reward, term, trunc, info = self.env.step(act)\n",
    "        self._buffer.on_step(act, next_obs, reward, term, trunc)\n",
    "        return next_obs, reward, term, trunc, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import typing\n",
    "import sys\n",
    "import logging\n",
    "import abc\n",
    "import torch.utils.tensorboard as tensorboard\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision.transforms.functional as tvF\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class QNetwork(Protocol):\n",
    "    num_actions: int\n",
    "\n",
    "    def __call__(self, obs: Tensor) -> Tensor:\n",
    "        ...\n",
    "\n",
    "class BaseQNetwork(nn.Module, QNetwork):\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__()\n",
    "        assert isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "        self.num_actions = int(env.action_space.n)\n",
    "        # self.main = nn.Sequential(\n",
    "        #     ObsEncoder(env.observation_space, 128),\n",
    "        #     MLP([128, 64, self.num_actions]),\n",
    "        # )\n",
    "        in_features = int(np.prod(env.observation_space.shape))\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.num_actions),\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs: Tensor):\n",
    "        return self.main(obs)\n",
    "\n",
    "class QAdvNetwork(nn.Module, QNetwork):\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__()\n",
    "        assert isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "        self.num_actions = int(env.action_space.n)\n",
    "        self.encoder = ObsEncoder(env.observation_space, 128)\n",
    "        self.adv_head = MLP([128, 32, 1])\n",
    "        self.q_head = MLP([128, 64, self.num_actions])\n",
    "    \n",
    "    def forward(self, obs: Tensor):\n",
    "        enc = self.encoder(obs)\n",
    "        adv = self.adv_head(enc)\n",
    "        qs = self.q_head(obs)\n",
    "        return adv + qs\n",
    "\n",
    "class Agent(Protocol):\n",
    "    def act(self, obs):\n",
    "        ...\n",
    "\n",
    "class GreedyAgent(Agent):\n",
    "    def __init__(self, Q: QNetwork):\n",
    "        self.Q = Q\n",
    "    \n",
    "    def batch_act(self, obs: Tensor) -> Tensor:\n",
    "        with torch.no_grad():\n",
    "            q_vals = self.Q(obs)\n",
    "        return torch.argmax(q_vals, 1)\n",
    "\n",
    "    def act(self, obs: Tensor) -> Tensor:\n",
    "        return self.batch_act(obs.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "class EpsGreedyAgent(Agent):\n",
    "    def __init__(self, Q: QNetwork, eps: float):\n",
    "        self.Q = Q\n",
    "        self.greedy = GreedyAgent(self.Q)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def batch_act(self, obs: Tensor) -> Tensor:\n",
    "        greedy_act = self.greedy.batch_act(obs)\n",
    "        rand_act = torch.randint(self.Q.num_actions, (len(obs),),\n",
    "                                 device=greedy_act.device)\n",
    "        rand_p = torch.rand(len(obs), device=greedy_act.device)\n",
    "        return torch.where(rand_p < self.eps, rand_act, greedy_act)\n",
    "\n",
    "    def act(self, obs: Tensor) -> Tensor:\n",
    "        return self.batch_act(obs.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    def __init__(self, env: gym.Env):\n",
    "        self.action_space = env.action_space\n",
    "    \n",
    "    def act(self, obs: typing.Any):\n",
    "        return self.action_space.sample()\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__()\n",
    "        assert isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "        self.Q = BaseQNetwork(env)\n",
    "        self.target_Q = BaseQNetwork(env)\n",
    "        self.target_Q.load_state_dict(self.Q.state_dict())\n",
    "\n",
    "class EpsScheduler:\n",
    "    def __init__(self, agent: EpsGreedyAgent, max_eps, min_eps, step_decay):\n",
    "        self.agent = agent\n",
    "        self.base_eps = min_eps\n",
    "        self.eps_amp = max_eps - min_eps\n",
    "        self.step_decay = step_decay\n",
    "        self.reset()\n",
    "\n",
    "        self.cur_eps = max_eps\n",
    "        agent.eps = self.cur_eps\n",
    "    \n",
    "    def reset(self):\n",
    "        self._cur_step = 0\n",
    "\n",
    "    def step(self):\n",
    "        cur_decay = np.exp(-self.step_decay * self._cur_step)\n",
    "        self.cur_eps = self.base_eps + self.eps_amp * cur_decay\n",
    "        self.agent.eps = self.cur_eps\n",
    "        self._cur_step += 1\n",
    "\n",
    "class DQNData(Protocol):\n",
    "    def train_env(self) -> gym.Env:\n",
    "        ...\n",
    "    \n",
    "    def val_env(self) -> gym.Env:\n",
    "        ...\n",
    "\n",
    "class DQNLoss:\n",
    "    def __init__(self, Q: QNetwork, target_Q: QNetwork, gamma: float):\n",
    "        self.Q = Q\n",
    "        self.target_Q = target_Q\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def __call__(self, batch: Transition):\n",
    "        value_preds = self.Q(batch.obs)\n",
    "        preds = value_preds.gather(1, batch.act.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_Q = self.target_Q(batch.next_obs)\n",
    "            next_V = next_Q.max(dim=1)[0]\n",
    "        done = batch.term | batch.trunc\n",
    "        targets = batch.reward + (1.0 - done.float()) * self.gamma * next_V\n",
    "\n",
    "        return F.smooth_l1_loss(preds, targets)\n",
    "\n",
    "class Rollout(data.IterableDataset[Transition]):\n",
    "    def __init__(self, env: gym.Env, agent, max_steps=None, max_episodes=None):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.max_steps = max_steps\n",
    "        self.max_episodes = max_episodes\n",
    "    \n",
    "    def __iter__(self):\n",
    "        ep_idx = step_idx = -1\n",
    "        obs = None\n",
    "        \n",
    "        # NOTE: This weird-ass while-loop structure is mainly designed so that\n",
    "        # after the episode has ended, it isn't reset immediately. If\n",
    "        # max_episodes is not None, we want the \"final state\" of env to remain\n",
    "        # when we exit the iterator.\n",
    "        while True:\n",
    "            if obs is None:\n",
    "                ep_idx += 1\n",
    "                if self.max_episodes is not None:\n",
    "                    if ep_idx >= self.max_episodes:\n",
    "                        break\n",
    "                \n",
    "                obs, info = self.env.reset()\n",
    "            \n",
    "            step_idx += 1\n",
    "            if self.max_steps is not None:\n",
    "                if step_idx >= self.max_steps:\n",
    "                    break\n",
    "            \n",
    "            act = self.agent.act(obs)\n",
    "            next_obs, reward, term, trunc, info = self.env.step(act)\n",
    "            yield Transition(obs, act, next_obs, reward, term, trunc)\n",
    "            obs = next_obs\n",
    "\n",
    "            if term or trunc:\n",
    "                obs = None\n",
    "\n",
    "class RandomInfiniteSampler:\n",
    "    def __init__(self, ds: typing.Sized):\n",
    "        self.ds = ds\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            yield np.random.randint(len(self.ds))\n",
    "\n",
    "class NullProfiler:\n",
    "    def start(self, *args, **kwargs): ...    \n",
    "    def step(self, *args, **kwargs): ...\n",
    "    def stop(self, *args, **kwargs): ...\n",
    "\n",
    "class NullWriter:\n",
    "    def add_scalar(self, *args, **kwargs): ...\n",
    "    def add_video(self, *args, **kwargs): ...\n",
    "\n",
    "class DQNTrainer:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 128\n",
    "        self.train_steps = int(1e6)\n",
    "        self.train_episodes = int(5e3)\n",
    "        self.val_every_steps = int(10e3)\n",
    "        self.val_episodes = 32\n",
    "        self.buffer_capacity = int(1e4)\n",
    "        self.max_eps, self.min_eps = 0.9, 0.05\n",
    "        self.eps_step_decay = 1e-3\n",
    "        self.val_eps = 0.05\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.995\n",
    "        self.clip_grad = 100.0\n",
    "        self.prefill = int(1e3)\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        \n",
    "        self.log = True\n",
    "        self.profile = True\n",
    "        self.progress_bar = True\n",
    "\n",
    "    def train(self, dqn: DQN, dqn_data: DQNData):\n",
    "        dqn = dqn.to(self.device)\n",
    "        \n",
    "        train_env = dqn_data.train_env()\n",
    "        train_agent = EpsGreedyAgent(dqn.Q, self.max_eps)\n",
    "\n",
    "        train_buffer = TransitionBuffer(train_env, self.buffer_capacity)\n",
    "        train_env = CollectTransitions(train_env, train_buffer)\n",
    "\n",
    "        # Online data from a rollout\n",
    "        train_rollout = Rollout(train_env, train_agent)\n",
    "        train_env_iter = iter(train_rollout)\n",
    "\n",
    "        # Offline data from the buffer\n",
    "        # NOTE to self: Can possibly replace it with the \"Dreamer\" buffer?\n",
    "        # Would also make it super-clean\n",
    "        train_buf_loader = data.DataLoader(\n",
    "            dataset=train_buffer,\n",
    "            sampler=data.BatchSampler(\n",
    "                sampler=RandomInfiniteSampler(train_buffer),\n",
    "                batch_size=self.batch_size,\n",
    "                drop_last=False,\n",
    "            ),\n",
    "            batch_size=None,\n",
    "        )\n",
    "        train_buf_iter = iter(train_buf_loader)\n",
    "\n",
    "        val_env = dqn_data.val_env()\n",
    "        val_agent = EpsGreedyAgent(dqn.Q, self.val_eps)\n",
    "\n",
    "        dqn_loss = DQNLoss(dqn.Q, dqn.target_Q, self.gamma)\n",
    "\n",
    "        optim = torch.optim.AdamW(dqn.Q.parameters(), lr=1e-4, amsgrad=True)\n",
    "        eps_sched = EpsScheduler(train_agent, self.max_eps, self.min_eps,\n",
    "                                 self.eps_step_decay)\n",
    "        polyak = Polyak(dqn.Q, dqn.target_Q, tau=self.tau)\n",
    "\n",
    "        step_idx = 0\n",
    "        \n",
    "        if self.profile:\n",
    "            prof_dir = f\"{self.writer.log_dir}_prof\"\n",
    "            prof_steps = int(1e3)\n",
    "            prof_wait, prof_warmup, prof_active = \\\n",
    "                np.array([1, 1, 3]) * prof_steps\n",
    "            prof_repeat = 2\n",
    "        \n",
    "            prof = torch.profiler.profile(\n",
    "                schedule=torch.profiler.schedule(\n",
    "                    wait=prof_wait, warmup=prof_warmup, active=prof_active, \n",
    "                    repeat=prof_repeat),\n",
    "                on_trace_ready=torch.profiler.tensorboard_trace_handler(prof_dir),\n",
    "                record_shapes=True,\n",
    "                with_stack=True)\n",
    "        else:\n",
    "            prof = NullProfiler()\n",
    "            \n",
    "        if self.log:\n",
    "            writer = tensorboard.SummaryWriter()\n",
    "        else:\n",
    "            writer = NullWriter()\n",
    "        \n",
    "        if self.progress_bar:\n",
    "            pbar = tqdm(desc=\"DQN\")\n",
    "\n",
    "        # Prefill the replay buffer\n",
    "        # while len(train_buffer) < self.prefill:\n",
    "        #     _ = next(train_env_iter)\n",
    "        # train_env.reset()\n",
    "\n",
    "        def train_step():\n",
    "            # Env interaction            \n",
    "            _ = next(train_env_iter)\n",
    "            \n",
    "            if len(train_buffer) < self.batch_size:\n",
    "                return\n",
    "            \n",
    "            # Policy learning\n",
    "            batch = next(train_buf_iter)\n",
    "            batch = batch.to(self.device)\n",
    "\n",
    "            loss = dqn_loss(batch)\n",
    "            \n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            if self.clip_grad is not None:\n",
    "                nn.utils.clip_grad.clip_grad_value_(\n",
    "                    dqn.Q.parameters(), self.clip_grad)\n",
    "            optim.step()\n",
    "\n",
    "            polyak.step()\n",
    "            eps_sched.step()\n",
    "            \n",
    "            writer.add_scalar(\"train/loss\", loss, global_step=step_idx)\n",
    "            writer.add_scalar(\"train/eps\", eps_sched.cur_eps,\n",
    "                                   global_step=step_idx)\n",
    "            writer.add_scalar(\"train/buffer\", len(train_buffer),\n",
    "                                   global_step=step_idx)\n",
    "            \n",
    "            prof.step()\n",
    "\n",
    "        def val_epoch():\n",
    "            all_ep_returns = []\n",
    "            for val_ep_idx in range(self.val_episodes):\n",
    "                cur_env = val_env\n",
    "                if val_ep_idx == 0:\n",
    "                    cur_env = gym.wrappers.RenderCollection(cur_env)\n",
    "                \n",
    "                val_ep = Rollout(cur_env, val_agent, max_episodes=1)\n",
    "                ep_returns = sum(tr.reward for tr in val_ep)\n",
    "                all_ep_returns.append(ep_returns)\n",
    "                if val_ep_idx == 0:\n",
    "                    val_frames = []\n",
    "                    for frame in cur_env.frame_list:\n",
    "                        frame_img = Image.fromarray(frame, mode=\"RGB\")\n",
    "                        val_frames.append(tvF.to_tensor(frame_img))\n",
    "                    # For some reason .add_video needs (N, L, C, H, W) tensor\n",
    "                    val_frames = torch.stack(val_frames).unsqueeze(0)\n",
    "                    video_fps = cur_env.metadata.get(\"render_fps\", 30.0)\n",
    "                \n",
    "            R = np.array(all_ep_returns)\n",
    "            writer.add_scalar(\"val/returns\", R.mean(), global_step=step_idx)\n",
    "            writer.add_video(\"val/video\", val_frames, global_step=step_idx,\n",
    "                                  fps=video_fps)\n",
    "            pbar.set_postfix({\"val/returns\": R.mean()})\n",
    "        \n",
    "        prof.start()\n",
    "        while True:\n",
    "            if step_idx % self.val_every_steps == 0:\n",
    "                val_epoch()\n",
    "                \n",
    "            if step_idx >= self.train_steps:\n",
    "                break\n",
    "\n",
    "            train_step()\n",
    "            \n",
    "            step_idx += 1\n",
    "            pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8392bef62fd147b28577dd4181a7fc7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DQN: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import TransformObservation\n",
    "\n",
    "\n",
    "class ToTensor(gym.Wrapper):\n",
    "    def __init__(self, env: gym.Env, device=None):\n",
    "        super().__init__(env)\n",
    "        self.device = device\n",
    "    \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        obs, info = self.env.reset(seed=seed, options=options)\n",
    "        obs = torch.as_tensor(obs, device=self.device)\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action: Tensor):\n",
    "        action = action.cpu().item()\n",
    "        obs, reward, term, trunc, info = self.env.step(action)\n",
    "        obs = torch.as_tensor(obs, device=self.device)\n",
    "        return obs, reward, term, trunc, info\n",
    "\n",
    "class DQNData_v0(DQNData):\n",
    "    def __init__(self, device=None):\n",
    "        self.device = device\n",
    "        \n",
    "    def train_env(self):\n",
    "        return self.val_env()\n",
    "    \n",
    "    def val_env(self):\n",
    "        env_name = \"LunarLander-v2\"\n",
    "        env = gym.make(env_name, render_mode=\"rgb_array\") \n",
    "        env = ToTensor(env, device=self.device)\n",
    "        env.reset(seed=42)\n",
    "        return env\n",
    "\n",
    "trainer = DQNTrainer()\n",
    "dqn_data = DQNData_v0(device=trainer.device)\n",
    "dqn = DQN(env=dqn_data.train_env())\n",
    "\n",
    "trainer.train(dqn, dqn_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
