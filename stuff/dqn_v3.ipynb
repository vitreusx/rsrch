{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import NamedTuple, List, Protocol, Any, Sequence, Optional, Protocol\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium.wrappers\n",
    "from rsrch.utils.polyak import Polyak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Sequential):\n",
    "    def __init__(self, num_features: List[int], norm_layer=nn.LayerNorm, act_layer=nn.ReLU):\n",
    "        layers = []\n",
    "        seq = enumerate(zip(num_features[:-1], num_features[1:]))\n",
    "        final_layer_idx = len(num_features) - 1\n",
    "        for layer_idx, (in_features, out_features) in seq:\n",
    "            if layer_idx > 0:\n",
    "                layers.append(norm_layer(in_features))\n",
    "                layers.append(act_layer())\n",
    "            bias = (layer_idx == final_layer_idx)\n",
    "            layers.append(nn.Linear(in_features, out_features, bias=bias))\n",
    "        \n",
    "        super().__init__(*layers)\n",
    "\n",
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self, obs_shape: torch.Size, kernel_size=3, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.obs_shape = obs_shape\n",
    "        self.kernel_size = k = kernel_size\n",
    "        self.hidden_dim = h = hidden_dim\n",
    "\n",
    "        c, W, H = self.obs_shape\n",
    "        assert W % 16 == 0 and H % 16 == 0, \\\n",
    "            \"image resolution should be divisible by 16\"\n",
    "        assert k % 2 == 1, \\\n",
    "            \"kernel_size should be an odd number\"\n",
    "        p = k // 2\n",
    "\n",
    "        final_size = torch.Size([H // 16, W // 16, 8*h])\n",
    "        self.out_features = final_size.numel()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(c, h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(h, 2*h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(2*h, 4*h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(4*h, 8*h, k, 1, p),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.main(x)\n",
    "\n",
    "class ObsEncoder(nn.Module):\n",
    "    def __init__(self, obs_space: gym.Space, enc_dim: int):\n",
    "        super().__init__()\n",
    "        self.out_features = enc_dim\n",
    "        \n",
    "        if isinstance(obs_space, gym.spaces.Box):\n",
    "            obs_shape = torch.Size(obs_space.shape)\n",
    "            if len(obs_shape) >= 3:\n",
    "                self.enc = VisualEncoder(obs_shape)\n",
    "                self.enc = nn.Sequential(self.enc, nn.Linear(self.enc.out_features, enc_dim))\n",
    "            else:\n",
    "                self.enc = MLP([obs_shape.numel(), 256, enc_dim])\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def forward(self, obs: Tensor) -> Tensor:\n",
    "        return self.enc(obs)\n",
    "\n",
    "class ActionEncoder(nn.Module):\n",
    "    def __init__(self, act_space: gym.Space, enc_dim: int):\n",
    "        super().__init__()\n",
    "        self.out_features = enc_dim\n",
    "\n",
    "        if isinstance(act_space, gym.spaces.Discrete):\n",
    "            self.enc = nn.Embedding(act_space.n, enc_dim)\n",
    "        elif isinstance(act_space, gym.spaces.Box):\n",
    "            shape = torch.Size(act_space.shape)\n",
    "            self.enc = MLP([shape.numel(), 256, enc_dim])\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    \n",
    "    def forward(self, act: Tensor) -> Tensor:\n",
    "        return self.enc(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventHandler(Protocol):\n",
    "    def on_reset(self, obs):\n",
    "        ...\n",
    "    \n",
    "    def on_step(self, act, next_obs, reward, term, trunc):\n",
    "        ...\n",
    "\n",
    "class InvokeHandler(gym.Wrapper):\n",
    "    def __init__(self, env: gym.Env, handler: EventHandler):\n",
    "        super().__init__(env)\n",
    "        self._handler = handler\n",
    "    \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        obs, info = self.env.reset(seed=seed, options=options)\n",
    "        self._handler.on_reset(obs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, act):\n",
    "        next_obs, reward, term, trunc, info = self.env.step(act)\n",
    "        self._handler.on_step(act, next_obs, reward, term, trunc)\n",
    "        return next_obs, reward, term, trunc, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(NamedTuple):\n",
    "    obs: Tensor\n",
    "    act: Tensor\n",
    "    next_obs: Tensor\n",
    "    reward: float\n",
    "    term: bool\n",
    "    trunc: bool\n",
    "\n",
    "class TransitionBatch(NamedTuple):\n",
    "    obs: Tensor\n",
    "    act: Tensor\n",
    "    next_obs: Tensor\n",
    "    reward: Tensor\n",
    "    term: Tensor\n",
    "    trunc: Tensor\n",
    "\n",
    "class TransitionBuffer(data.Dataset):\n",
    "    def __init__(self, env: gym.Env, capacity: int):\n",
    "        def buffer_for(space=None, dtype=None):\n",
    "            if space is not None:\n",
    "                x = np.empty(space.shape, dtype=space.dtype)\n",
    "                x = torch.from_numpy(x)\n",
    "                x = torch.empty(capacity, *x.shape, dtype=x.dtype)\n",
    "            else:\n",
    "                x = torch.empty(capacity, dtype=dtype)\n",
    "            return x\n",
    "\n",
    "        self.obs = buffer_for(space=env.observation_space)\n",
    "        self.act = buffer_for(space=env.action_space)\n",
    "        self.next_obs = torch.empty_like(self.obs)\n",
    "        self.reward = buffer_for(dtype=torch.float)\n",
    "        self.term = buffer_for(dtype=torch.bool)\n",
    "        self.trunc = buffer_for(dtype=torch.bool)\n",
    "\n",
    "        self._capacity = capacity\n",
    "        self._cursor = self._size = 0\n",
    "        self._cur_obs = None\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.obs.device\n",
    "    \n",
    "    def _convert(self, x, type_as):\n",
    "        return torch.as_tensor(x).type_as(type_as)\n",
    "    \n",
    "    def push(self, obs, act, next_obs, reward, term, trunc):\n",
    "        idx = self._cursor\n",
    "        self.obs[idx] = self._convert(obs, self.obs)\n",
    "        self.act[idx] = self._convert(act, self.act)\n",
    "        self.next_obs[idx] = self._convert(next_obs, self.next_obs)\n",
    "        self.reward[idx] = reward\n",
    "        self.term[idx] = term\n",
    "        self.trunc[idx] = trunc\n",
    "\n",
    "        self._cursor = (self._cursor + 1) % self._capacity\n",
    "        if self._size < self._capacity:\n",
    "            self._size += 1\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return Transition(\n",
    "            obs=self.obs[idx],\n",
    "            act=self.act[idx],\n",
    "            next_obs=self.next_obs[idx],\n",
    "            reward=self.reward[idx],\n",
    "            term=self.term[idx],\n",
    "            trunc=self.trunc[idx],\n",
    "        )\n",
    "\n",
    "class CollectTransitions(gym.Wrapper):\n",
    "    def __init__(self, env: gym.Env, buffer: TransitionBuffer):\n",
    "        super().__init__(env)\n",
    "        self._buffer = buffer\n",
    "    \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        obs, info = self.env.reset(seed=seed, options=options)\n",
    "        self._obs = obs\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, act):\n",
    "        next_obs, reward, term, trunc, info = self.env.step(act)\n",
    "        self._buffer.push(self._obs, act, next_obs, reward, term, trunc)\n",
    "        return next_obs, reward, term, trunc, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trajectory(Protocol):\n",
    "    obs: Sequence\n",
    "    act: Sequence\n",
    "    reward: Sequence\n",
    "    trunc: Sequence\n",
    "    term: Sequence\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        ...\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ...\n",
    "\n",
    "class TensorTrajectory(NamedTuple):\n",
    "    obs: Tensor\n",
    "    act: Tensor\n",
    "    reward: Tensor\n",
    "    trunc: Tensor\n",
    "    term: Tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def as_tensor(tr: Trajectory):\n",
    "        return TensorTrajectory(\n",
    "            obs=torch.tensor(tr.obs),\n",
    "            act=torch.tensor(tr.act),\n",
    "            reward=torch.tensor(tr.reward),\n",
    "            trunc=torch.tensor(tr.trunc),\n",
    "            term=torch.tensor(tr.term),\n",
    "        )\n",
    "\n",
    "    def clone(self):\n",
    "        return TensorTrajectory(\n",
    "            obs=self.obs.clone(),\n",
    "            act=self.act.clone(),\n",
    "            reward=self.reward.clone(),\n",
    "            trunc=self.trunc.clone(),\n",
    "            term=self.term.clone(),\n",
    "        )\n",
    "\n",
    "    def to(self, device: torch.device):\n",
    "        return TensorTrajectory(\n",
    "            obs=self.obs.to(device),\n",
    "            act=self.act.to(device),\n",
    "            reward=self.reward.to(device),\n",
    "            trunc=self.trunc.to(device),\n",
    "            term=self.term.to(device),\n",
    "        )\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return TensorTrajectory(\n",
    "            obs=self.obs[idx],\n",
    "            act=self.act[idx],\n",
    "            reward=self.reward[idx],\n",
    "            trunc=self.trunc[idx],\n",
    "            term=self.term[idx],\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.obs)\n",
    "\n",
    "class MmapTrajectory(NamedTuple):\n",
    "    obs: np.memmap\n",
    "    act: np.memmap\n",
    "    reward: np.memmap\n",
    "    trunc: np.memmap\n",
    "    term: np.memmap\n",
    "    \n",
    "    @staticmethod\n",
    "    def open(root: Path):\n",
    "        return MmapTrajectory(\n",
    "            obs=np.load(root / \"obs.npy\", mmap_mode=\"r\"),\n",
    "            act=np.load(root / \"act.npy\", mmap_mode=\"r\"),\n",
    "            reward=np.load(root / \"reward.npy\", mmap_mode=\"r\"),\n",
    "            trunc=np.load(root / \"trunc.npy\", mmap_mode=\"r\"),\n",
    "            term=np.load(root / \"term.npy\", mmap_mode=\"r\"),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def save(tr: Trajectory, root: Path):\n",
    "        root.mkdir(parents=True, exist_ok=True)\n",
    "        np.save(root / \"obs.npy\", np.asarray(tr.obs))\n",
    "        np.save(root / \"act.npy\", np.asarray(tr.act))\n",
    "        np.save(root / \"reward.npy\", np.asarray(tr.reward))\n",
    "        np.save(root / \"trunc.npy\", np.asarray(tr.trunc))\n",
    "        np.save(root / \"term.npy\", np.asarray(tr.term))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.obs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return MmapTrajectory(obs=self.obs[idx], act=self.act[idx],\n",
    "                              reward=self.reward[idx], trunc=self.trunc[idx],\n",
    "                              term=self.term[idx])\n",
    "\n",
    "class Subsample(nn.Module):\n",
    "    def __init__(self, seq_len: int):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def forward(self, traj: Trajectory):\n",
    "        start = np.random.randint(len(traj))\n",
    "        end = start + self.seq_len\n",
    "        return traj[start:end]\n",
    "\n",
    "class ToTensor(nn.Module):\n",
    "    def forward(self, traj: Trajectory):\n",
    "        return TensorTrajectory.as_tensor(traj)\n",
    "\n",
    "class MapDs(data.Dataset):\n",
    "    def __init__(self, ds: data.Dataset, f):\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.f = f\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.f(self.ds[idx])\n",
    "\n",
    "class MapIterDs(data.IterableDataset):\n",
    "    def __init__(self, ds: data.IterableDataset, f):\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.f = f\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for x in self.ds:\n",
    "            yield self.f(x)\n",
    "\n",
    "class TrajectoryBatch(NamedTuple):\n",
    "    obs: rnn.PackedSequence\n",
    "    act: rnn.PackedSequence\n",
    "    reward: rnn.PackedSequence\n",
    "    trunc: rnn.PackedSequence\n",
    "    term: rnn.PackedSequence\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch: List[TensorTrajectory]) -> TrajectoryBatch:\n",
    "        lengths = torch.as_tensor([len(tr) for tr in batch])\n",
    "        idxes = torch.argsort(lengths, descending=True)\n",
    "        \n",
    "        return TrajectoryBatch(\n",
    "            obs=rnn.pack_sequence([batch[idx].obs for idx in idxes]),\n",
    "            act=rnn.pack_sequence([batch[idx].act for idx in idxes]),\n",
    "            reward=rnn.pack_sequence([batch[idx].reward for idx in idxes]),\n",
    "            trunc=rnn.pack_sequence([batch[idx].trunc for idx in idxes]),\n",
    "            term=rnn.pack_sequence([batch[idx].term for idx in idxes]),\n",
    "        )\n",
    "\n",
    "class EpisodeBuffer(data.Dataset):\n",
    "    def __init__(self, env: gym.Env, max_seq_len: int, seq_capacity: int, mmap_root: Optional[Path] = None):\n",
    "        def buffer_for(space=None, dtype=None):\n",
    "            if space is not None:\n",
    "                x = np.empty(space.shape, dtype=space.dtype)\n",
    "                x = torch.from_numpy(x)\n",
    "                x = torch.empty(max_seq_len, *x.shape, dtype=x.dtype)\n",
    "            else:\n",
    "                x = torch.empty(max_seq_len, dtype=dtype)\n",
    "            return x\n",
    "\n",
    "        self.obs = buffer_for(space=env.observation_space)\n",
    "        self.act = buffer_for(space=env.action_space)\n",
    "        self.reward = buffer_for(dtype=torch.float)\n",
    "        self.trunc = buffer_for(dtype=torch.bool)\n",
    "        self.trunc.fill_(True)\n",
    "        self.term = buffer_for(dtype=torch.bool)\n",
    "        \n",
    "        self._episodes = np.empty((seq_capacity,), dtype=object)\n",
    "        self.mmap_root = mmap_root\n",
    "\n",
    "        self._cur_step = 0\n",
    "        self._cur_ep_idx = -1\n",
    "        self.num_episodes = 0\n",
    "        self.seq_capacity = seq_capacity\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.obs.device\n",
    "    \n",
    "    def _conv(self, x, type_as):\n",
    "        return torch.as_tensor(x).type_as(type_as)\n",
    "\n",
    "    def on_reset(self, obs):\n",
    "        self._cur_step = self._cur_ep_len = 0\n",
    "        self._cur_ep_idx = (self._cur_ep_idx + 1) % self.seq_capacity\n",
    "        self.num_episodes = min(self.num_episodes + 1, self.seq_capacity)\n",
    "\n",
    "        self.obs[self._cur_step] = self._conv(obs, self.obs)\n",
    "        self.trunc[self._cur_step] = True\n",
    "        self.term[self._cur_step] = False\n",
    "\n",
    "        self._update_cur_episode_view()\n",
    "    \n",
    "    def _update_cur_episode_view(self):\n",
    "        ep_len = self._cur_step + 1\n",
    "        \n",
    "        # NOTE: This only creates a view, no data is copied\n",
    "        self._episodes[self._cur_ep_idx] = TensorTrajectory(\n",
    "            obs=self.obs[:ep_len],\n",
    "            act=self.act[:ep_len],\n",
    "            reward=self.reward[:ep_len],\n",
    "            trunc=self.trunc[:ep_len],\n",
    "            term=self.term[:ep_len],\n",
    "        )\n",
    "    \n",
    "    def on_step(self, act, next_obs, reward, term, trunc):\n",
    "        if self._cur_step < self.max_seq_len:\n",
    "            self.act[self._cur_step] = self._conv(act, self.act)\n",
    "            self.trunc[self._cur_step] = False\n",
    "            self._cur_step += 1\n",
    "            self.obs[self._cur_step] = self._conv(next_obs, self.obs)\n",
    "            self.reward[self._cur_step] = reward\n",
    "            self.term[self._cur_step] = term\n",
    "            self.trunc[self._cur_step] = trunc\n",
    "\n",
    "        self._update_cur_episode_view()\n",
    "\n",
    "        done = term or trunc\n",
    "        if done:\n",
    "            cur_ep_view = self._episodes[self._cur_ep_idx]\n",
    "            if self.mmap_root is not None:\n",
    "                dst_root = self.mmap_root / f\"{self._cur_ep_idx:06d}\"\n",
    "                MmapTrajectory.save(cur_ep_view, dst_root)\n",
    "                self._episodes[self._cur_ep_idx] = MmapTrajectory.open(dst_root)\n",
    "            else:\n",
    "                self._episodes[self._cur_ep_idx] = cur_ep_view.clone()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_episodes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= self.num_episodes:\n",
    "            raise IndexError()\n",
    "        return self._episodes[idx]\n",
    "\n",
    "\n",
    "class CollectEpisodes(gym.Wrapper):\n",
    "    def __init__(self, env: gym.Env, buffer: EpisodeBuffer):\n",
    "        super().__init__(env)\n",
    "        self._buffer = buffer\n",
    "    \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        obs, info = self.env.reset(seed=seed, options=options)\n",
    "        self._buffer.on_reset(obs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, act):\n",
    "        next_obs, reward, term, trunc, info = self.env.step(act)\n",
    "        self._buffer.on_step(act, next_obs, reward, term, trunc)\n",
    "        return next_obs, reward, term, trunc, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import typing\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "class QNetwork(Protocol):\n",
    "    num_actions: int\n",
    "\n",
    "    def __call__(self, obs: Tensor) -> Tensor:\n",
    "        ...\n",
    "\n",
    "class BaseQNetwork(nn.Module, QNetwork):\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__()\n",
    "        assert isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "        self.num_actions = int(env.action_space.n)\n",
    "        self.main = nn.Sequential(\n",
    "            ObsEncoder(env.observation_space, 128),\n",
    "            MLP([128, 64, self.num_actions]),\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs: Tensor):\n",
    "        return self.main(obs)\n",
    "\n",
    "class QAdvNetwork(nn.Module, QNetwork):\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__()\n",
    "        assert isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "        self.num_actions = int(env.action_space.n)\n",
    "        self.encoder = ObsEncoder(env.observation_space, 128)\n",
    "        self.adv_head = MLP([128, 32, 1])\n",
    "        self.q_head = MLP([128, 64, self.num_actions])\n",
    "    \n",
    "    def forward(self, obs: Tensor):\n",
    "        enc = self.encoder(obs)\n",
    "        adv = self.adv_head(enc)\n",
    "        qs = self.q_head(obs)\n",
    "        return adv + qs\n",
    "\n",
    "class GreedyAgent:\n",
    "    def __init__(self, Q: QNetwork):\n",
    "        self.Q = Q\n",
    "    \n",
    "    def batch_act(self, obs: Tensor) -> Tensor:\n",
    "        with torch.no_grad():\n",
    "            q_vals = self.Q(obs)\n",
    "        return torch.argmax(q_vals, 1)\n",
    "\n",
    "    def act(self, obs: Tensor) -> Tensor:\n",
    "        return self.batch_act(obs.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "class EpsGreedyAgent:\n",
    "    def __init__(self, Q: QNetwork, eps: float):\n",
    "        self.Q = Q\n",
    "        self.greedy = GreedyAgent(self.Q)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def batch_act(self, obs: Tensor) -> Tensor:\n",
    "        rand_p = torch.rand(len(obs), device=obs.device)\n",
    "        rand_act = torch.randint(self.Q.num_actions, (len(obs),))\n",
    "        greedy_act = self.greedy.batch_act(obs)\n",
    "        return torch.where(rand_p < self.eps, rand_act, greedy_act)\n",
    "\n",
    "    def act(self, obs: Tensor) -> Tensor:\n",
    "        return self.batch_act(obs.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "class RandomAgent:\n",
    "    def __init__(self, env: gym.Env):\n",
    "        self.action_space = env.action_space\n",
    "    \n",
    "    def __call__(self, obs):\n",
    "        return self.action_space.sample()\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__()\n",
    "        assert isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "        self.Q = BaseQNetwork(env)\n",
    "        self.target_Q = BaseQNetwork(env)\n",
    "        self.target_Q.load_state_dict(self.Q.state_dict())\n",
    "\n",
    "class EpsScheduler:\n",
    "    def __init__(self, agent: EpsGreedyAgent, max_eps, min_eps, step_decay):\n",
    "        self.agent = agent\n",
    "        self.base_eps = min_eps\n",
    "        self.eps_amp = max_eps - min_eps\n",
    "        self.step_decay = step_decay\n",
    "        self.reset()\n",
    "\n",
    "        self.cur_eps = max_eps\n",
    "        agent.eps = self.cur_eps\n",
    "    \n",
    "    def reset(self):\n",
    "        self._cur_step = 0\n",
    "\n",
    "    def step(self):\n",
    "        cur_decay = np.exp(-self.step_decay * self._cur_step)\n",
    "        self.cur_eps = self.base_eps + self.eps_amp * cur_decay\n",
    "        self.agent.eps = self.cur_eps\n",
    "        self._cur_step += 1\n",
    "\n",
    "class DQNData(Protocol):\n",
    "    def train_env(self) -> gym.Env:\n",
    "        ...\n",
    "    \n",
    "    def val_env(self) -> gym.Env:\n",
    "        ...\n",
    "\n",
    "class DQNLoss:\n",
    "    def __init__(self, Q: QNetwork, target_Q: QNetwork, gamma: float):\n",
    "        self.Q = Q\n",
    "        self.target_Q = target_Q\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def __call__(self, batch: TransitionBatch):\n",
    "        value_preds = self.Q(batch.obs)\n",
    "        preds = value_preds.gather(1, batch.act.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_V = self.target_Q(batch.next_obs).max(dim=1)[0]\n",
    "        targets = batch.reward + (1.0 - batch.term.float()) * self.gamma * next_V\n",
    "\n",
    "        return F.smooth_l1_loss(preds, targets)\n",
    "\n",
    "class Rollout(data.IterableDataset[Transition]):\n",
    "    def __init__(self, env: gym.Env, agent, max_steps=None, max_episodes=None):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.max_steps = max_steps\n",
    "        self.max_episodes = max_episodes\n",
    "    \n",
    "    def __iter__(self):\n",
    "        obs, info = self.env.reset()\n",
    "\n",
    "        ep_idx = step_idx = 0\n",
    "        \n",
    "        while True:\n",
    "            if self.max_steps is not None and step_idx >= self.max_steps:\n",
    "                break\n",
    "\n",
    "            if self.max_episodes is not None and ep_idx >= self.max_episodes:\n",
    "                break\n",
    "            \n",
    "            act = self.agent.act(obs)\n",
    "            next_obs, reward, term, trunc, info = self.env.step(act)\n",
    "            yield Transition(obs, act, next_obs, reward, term, trunc)\n",
    "            obs = next_obs\n",
    "            step_idx += 1\n",
    "\n",
    "            if term or trunc:\n",
    "                obs, info = self.env.reset()\n",
    "                ep_idx += 1\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, level: int):\n",
    "        self.metric = lambda *args, **kwargs: ...\n",
    "\n",
    "class RandomInfiniteSampler:\n",
    "    def __init__(self, ds: typing.Sized):\n",
    "        self.ds = ds\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            yield np.random.randint(len(self.ds))\n",
    "\n",
    "class DQNTrainer:\n",
    "    def __init__(self):\n",
    "        self.train_steps = int(1e6)\n",
    "        self.train_episodes = int(5e3)\n",
    "        self.val_every_steps = int(10e3)\n",
    "        self.val_episodes = 32\n",
    "        self.buffer_capacity = int(1e5)\n",
    "        self.max_eps, self.min_eps = 0.9, 0.05\n",
    "        self.eps_step_decay = 1e-3\n",
    "        self.val_eps = 0.05\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.995\n",
    "        self.batch_size = 128\n",
    "        self.clip_grad = 100.0\n",
    "        self.prefill = int(1e3)\n",
    "\n",
    "        self.log = Logger(level=logging.WARN)\n",
    "\n",
    "    def train(self, dqn: DQN, dqn_data: DQNData):\n",
    "        train_env = dqn_data.train_env()\n",
    "        train_agent = EpsGreedyAgent(dqn.Q, self.max_eps)\n",
    "\n",
    "        train_buffer = TransitionBuffer(train_env, self.buffer_capacity)\n",
    "        train_env = CollectTransitions(train_env, train_buffer)\n",
    "\n",
    "        # Online data from a rollout\n",
    "        train_rollout = Rollout(train_env, train_agent)\n",
    "        train_env_iter = iter(train_rollout)\n",
    "\n",
    "        # Offline data from the buffer\n",
    "        # NOTE to self: Can possibly replace it with the \"Dreamer\" buffer?\n",
    "        # Would also make it super-clean\n",
    "        train_loader = data.DataLoader(\n",
    "            dataset=train_buffer,\n",
    "            sampler=data.BatchSampler(\n",
    "                sampler=RandomInfiniteSampler(train_buffer),\n",
    "                batch_size=self.batch_size,\n",
    "                drop_last=False,\n",
    "            ),\n",
    "            batch_size=None,\n",
    "        )\n",
    "        train_loader_iter = iter(train_loader)\n",
    "\n",
    "        val_env = dqn_data.val_env()\n",
    "        val_agent = EpsGreedyAgent(dqn.Q, self.val_eps)\n",
    "\n",
    "        dqn_loss = DQNLoss(dqn.Q, dqn.target_Q, self.gamma)\n",
    "\n",
    "        optim = torch.optim.AdamW(dqn.Q.parameters(), lr=1e-4, amsgrad=True)\n",
    "        eps_sched = EpsScheduler(train_agent, self.max_eps, self.min_eps,\n",
    "                                 self.eps_step_decay)\n",
    "        polyak = Polyak(dqn.Q, dqn.target_Q, tau=self.tau)\n",
    "\n",
    "        step_idx = 0\n",
    "\n",
    "        # Prefill the replay buffer\n",
    "        while len(train_buffer) < self.prefill:\n",
    "            _ = next(train_env_iter)\n",
    "        train_env.reset()\n",
    "\n",
    "        def train_step():\n",
    "            # Env interaction\n",
    "            _ = next(train_env_iter)\n",
    "            \n",
    "            # Policy learning\n",
    "            batch = next(train_loader_iter)\n",
    "\n",
    "            loss = dqn_loss(batch)\n",
    "                \n",
    "            optim.zero_grad(True)\n",
    "            loss.backward()\n",
    "            if self.clip_grad is not None:\n",
    "                nn.utils.clip_grad.clip_grad_value_(\n",
    "                    dqn.Q.parameters(), self.clip_grad)\n",
    "            optim.step()\n",
    "\n",
    "            polyak.step()\n",
    "            eps_sched.step()\n",
    "\n",
    "            self.log.metric(\"train_loss\", loss)\n",
    "            self.log.metric(\"cur_train_eps\", eps_sched.cur_eps)\n",
    "\n",
    "        def val_epoch():\n",
    "            all_ep_returns = []\n",
    "            for _ in range(self.val_episodes):\n",
    "                ep_returns = 0.0\n",
    "                for tr in Rollout(val_env, val_agent, max_episodes=1):\n",
    "                    ep_returns += tr.reward\n",
    "\n",
    "                all_ep_returns.append(ep_returns)\n",
    "            \n",
    "            self.log.metric(\"val_returns\", all_ep_returns)\n",
    "\n",
    "        while True:\n",
    "            if step_idx % self.val_every_steps == 0:\n",
    "                val_epoch()\n",
    "            \n",
    "            if step_idx >= self.train_steps:\n",
    "                break\n",
    "\n",
    "            train_step()\n",
    "            step_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m dqn \u001b[39m=\u001b[39m DQN(env\u001b[39m=\u001b[39mdqn_data\u001b[39m.\u001b[39mtrain_env())\n\u001b[1;32m     19\u001b[0m trainer \u001b[39m=\u001b[39m DQNTrainer()\n\u001b[0;32m---> 21\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(dqn, dqn_data)\n",
      "Cell \u001b[0;32mIn[6], line 270\u001b[0m, in \u001b[0;36mDQNTrainer.train\u001b[0;34m(self, dqn, dqn_data)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39mif\u001b[39;00m step_idx \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_steps:\n\u001b[1;32m    268\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m train_step()\n\u001b[1;32m    271\u001b[0m step_idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 244\u001b[0m, in \u001b[0;36mDQNTrainer.train.<locals>.train_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_grad \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad\u001b[39m.\u001b[39mclip_grad_value_(\n\u001b[1;32m    243\u001b[0m         dqn\u001b[39m.\u001b[39mQ\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_grad)\n\u001b[0;32m--> 244\u001b[0m optim\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    246\u001b[0m polyak\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    247\u001b[0m eps_sched\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/mambaforge/envs/rsrch/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/rsrch/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/mambaforge/envs/rsrch/lib/python3.10/site-packages/torch/optim/adamw.py:171\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    161\u001b[0m         group,\n\u001b[1;32m    162\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m         state_steps,\n\u001b[1;32m    169\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m     adamw(\n\u001b[1;32m    172\u001b[0m         params_with_grad,\n\u001b[1;32m    173\u001b[0m         grads,\n\u001b[1;32m    174\u001b[0m         exp_avgs,\n\u001b[1;32m    175\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    176\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    177\u001b[0m         state_steps,\n\u001b[1;32m    178\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    179\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    180\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    181\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    182\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    183\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    184\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    185\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    186\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    187\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    188\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    189\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    190\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/mambaforge/envs/rsrch/lib/python3.10/site-packages/torch/optim/adamw.py:321\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 321\u001b[0m func(\n\u001b[1;32m    322\u001b[0m     params,\n\u001b[1;32m    323\u001b[0m     grads,\n\u001b[1;32m    324\u001b[0m     exp_avgs,\n\u001b[1;32m    325\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    326\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    327\u001b[0m     state_steps,\n\u001b[1;32m    328\u001b[0m     amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    329\u001b[0m     beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    330\u001b[0m     beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    331\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    332\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    333\u001b[0m     eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    334\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    335\u001b[0m     capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    336\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    337\u001b[0m     grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    338\u001b[0m     found_inf\u001b[39m=\u001b[39;49mfound_inf,\n\u001b[1;32m    339\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/rsrch/lib/python3.10/site-packages/torch/optim/adamw.py:389\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    386\u001b[0m param\u001b[39m.\u001b[39mmul_(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m lr \u001b[39m*\u001b[39m weight_decay)\n\u001b[1;32m    388\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m    390\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad, value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    392\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import TransformObservation\n",
    "\n",
    "class TensorAction(gym.Wrapper):\n",
    "    def step(self, action: Any):\n",
    "        return self.env.step(action.item())\n",
    "\n",
    "class DQNData_v0(DQNData):\n",
    "    def train_env(self):\n",
    "        return self.val_env()\n",
    "    \n",
    "    def val_env(self):\n",
    "        env = gym.make(\"CartPole-v1\") \n",
    "        env = TensorAction(env)\n",
    "        env = TransformObservation(env, torch.as_tensor)\n",
    "        return env\n",
    "\n",
    "dqn_data = DQNData_v0()\n",
    "dqn = DQN(env=dqn_data.train_env())\n",
    "trainer = DQNTrainer()\n",
    "\n",
    "trainer.train(dqn, dqn_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
