{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94cadf41c3840a08e19d2f7ef6437d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 265\u001b[0m\n\u001b[1;32m    261\u001b[0m pix \u001b[39m=\u001b[39m PixelCNN(pix_data\u001b[39m.\u001b[39min_channels, pix_data\u001b[39m.\u001b[39mnum_values,\n\u001b[1;32m    262\u001b[0m                num_hidden_layers\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, hidden_dim\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m    263\u001b[0m trainer \u001b[39m=\u001b[39m Trainer()\n\u001b[0;32m--> 265\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(pix, pix_data)\n",
      "Cell \u001b[0;32mIn[3], line 216\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, pix, pix_data)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mfor\u001b[39;00m images \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m    215\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 216\u001b[0m     value_dist: Categorical \u001b[39m=\u001b[39m pix(images)\n\u001b[1;32m    217\u001b[0m     targets \u001b[39m=\u001b[39m (\u001b[39m255\u001b[39m \u001b[39m*\u001b[39m images)\u001b[39m.\u001b[39mint()\n\u001b[1;32m    218\u001b[0m     loss: Tensor \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mvalue_dist\u001b[39m.\u001b[39mlog_prob(targets)\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/mambaforge/envs/rsrch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 120\u001b[0m, in \u001b[0;36mPixelCNN.forward\u001b[0;34m(self, x, ch_idx)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m ch_idx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     outs \u001b[39m=\u001b[39m outs[:, \u001b[39m0\u001b[39m]\n\u001b[0;32m--> 120\u001b[0m outs \u001b[39m=\u001b[39m Categorical(logits\u001b[39m=\u001b[39;49mouts)\n\u001b[1;32m    121\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/mambaforge/envs/rsrch/lib/python3.10/site-packages/torch/distributions/categorical.py:66\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_events \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     65\u001b[0m batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39mndimension() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mSize()\n\u001b[0;32m---> 66\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m~/mambaforge/envs/rsrch/lib/python3.10/site-packages/torch/distributions/distribution.py:61\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     59\u001b[0m         value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, param)\n\u001b[1;32m     60\u001b[0m         valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39mcheck(value)\n\u001b[0;32m---> 61\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[1;32m     62\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m             )\n\u001b[1;32m     69\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple, Union\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "from pathlib import Path\n",
    "import torch.utils.data as data\n",
    "from rsrch.datasets import font_awesome, tiny_imagenet\n",
    "import rsrch.utils.data as data\n",
    "import torch.utils.tensorboard as tensorboard\n",
    "from contextlib import contextmanager\n",
    "import rsrch.utils.visual as visual\n",
    "import torchvision.transforms.functional as tv_F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask: Tensor, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mask: Tensor\n",
    "        self.register_buffer(\"mask\", mask.type_as(self.weight))\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        self.weight.data *= self.mask\n",
    "        return super().forward(x)\n",
    "\n",
    "\n",
    "class MaskedConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, mask: Tensor,\n",
    "                 k=3, norm_layer=nn.BatchNorm2d, act_layer=nn.ReLU):\n",
    "        super().__init__()\n",
    "        \n",
    "        p = k // 2\n",
    "        self.conv = MaskedConv2d(mask, in_channels, out_channels, k, 1, p)\n",
    "        self.norm = norm_layer(out_channels)\n",
    "        self.act = act_layer()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.act(self.norm(self.conv(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MaskFactory:\n",
    "    def input_layer_mask(self, in_channels: int, channel_idx: int,\n",
    "                   kernel_size: Tuple[int, int]):\n",
    "        kh, kw = kernel_size\n",
    "        mask = torch.ones((in_channels, kh, kw))\n",
    "        mask[channel_idx:, kh//2, kw//2] = 0\n",
    "        mask[:, kh//2, (kw//2+1):] = 0\n",
    "        mask[:, (kh//2+1):, :] = 0\n",
    "        return mask\n",
    "\n",
    "    def hidden_layer_mask(self, kernel_size: Tuple[int, int]):\n",
    "        kh, kw = kernel_size\n",
    "        mask = torch.ones((kh, kw))\n",
    "        mask[kh//2, (kw//2+1):] = 0\n",
    "        mask[(kh//2+1):, :] = 0\n",
    "        return mask\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def eval_ctx(net: nn.Module):\n",
    "    prev_val = net.training\n",
    "    net.train = False\n",
    "    with torch.no_grad():\n",
    "        yield\n",
    "    net.train = prev_val\n",
    "\n",
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, in_channels: int, num_values: int, num_hidden_layers: int,\n",
    "                 hidden_dim: int, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_values = num_values\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = self.k = kernel_size\n",
    "        \n",
    "        self.subnets = nn.ModuleList([\n",
    "            self._make_channel_subnet(ch_idx)\n",
    "            for ch_idx in range(self.in_channels)\n",
    "        ])\n",
    "    \n",
    "    def _make_channel_subnet(self, ch_idx: int):\n",
    "        input_mask = MaskFactory().input_layer_mask(\n",
    "            in_channels=self.in_channels, channel_idx=ch_idx,\n",
    "            kernel_size=(self.k, self.k))\n",
    "        input_layer = MaskedConvBlock(\n",
    "            in_channels=self.in_channels, out_channels=self.hidden_dim,\n",
    "            mask=input_mask, k=self.k)\n",
    "        \n",
    "        hidden_mask = MaskFactory().hidden_layer_mask(\n",
    "            kernel_size=(self.k, self.k))\n",
    "        hidden_layers = [\n",
    "            MaskedConvBlock(in_channels=self.hidden_dim, \n",
    "                            out_channels=self.hidden_dim,\n",
    "                            mask=hidden_mask,\n",
    "                            k=self.k)\n",
    "            for _ in range(self.num_hidden_layers)\n",
    "        ]\n",
    "        \n",
    "        p = self.kernel_size // 2\n",
    "        final_layer = nn.Conv2d(self.hidden_dim, self.num_values,\n",
    "                                self.k, 1, p)\n",
    "        \n",
    "        return nn.Sequential(input_layer, *hidden_layers, final_layer)\n",
    "    \n",
    "    def forward(self, x: Tensor, ch_idx=None):        \n",
    "        if ch_idx is None:\n",
    "            outs = [self.subnets[idx](x) for idx in range(self.in_channels)]\n",
    "        else:\n",
    "            outs = [self.subnets[ch_idx](x)]\n",
    "        \n",
    "        outs = torch.stack(outs, dim=1) # [B, C_in or 1, N_v, H, W]\n",
    "        outs = outs.permute(0, 1, 3, 4, 2) # [B, C_in or 1, H, W, N_v]\n",
    "        if ch_idx is not None:\n",
    "            outs = outs[:, 0]\n",
    "        outs = Categorical(logits=outs)\n",
    "        return outs\n",
    "\n",
    "    def predict(self, images: Tensor, start_pos: Tuple[int, int]):\n",
    "        ix0, iy0 = start_pos\n",
    "        _, c_in, h, w = images.shape\n",
    "        \n",
    "        result = images.clone()\n",
    "        \n",
    "        with eval_ctx(self):\n",
    "            for iy, ix, ic in np.ndindex((h, w, c_in)):\n",
    "                if (iy, ix) < (iy0, ix0):\n",
    "                    continue\n",
    "                \n",
    "                # Here we obtain Categorical(num_values) over [B, H, W]\n",
    "                value_dist: Categorical = self(result, ch_idx=ic)\n",
    "                logits_at_point = value_dist.logits[:, iy, ix] # [B, N_v]\n",
    "                preds = logits_at_point.argmax(-1)\n",
    "                result[:, ic, iy, ix] = preds\n",
    "        \n",
    "        return result\n",
    "                \n",
    "\n",
    "class PixelCNNData:\n",
    "    def __init__(self):\n",
    "        self._setup_tiny_imagenet()\n",
    "        # self._setup_font_awesome()\n",
    "    \n",
    "    def _setup_tiny_imagenet(self):\n",
    "        ds_root = \"../datasets/tiny-imagenet-200\"\n",
    "        self.train_ds = tiny_imagenet.TinyImageNet(root=ds_root, split=\"train\")\n",
    "        self.val_ds = tiny_imagenet.TinyImageNet(root=ds_root, split=\"val\")\n",
    "\n",
    "        def val_transform(item: font_awesome.Item) -> Tensor:\n",
    "            image = item.image\n",
    "            image = image.convert(\"RGB\")\n",
    "            image = tv_F.center_crop(image, (32, 32))\n",
    "            image = tv_F.to_tensor(image)\n",
    "            return image\n",
    "\n",
    "        def train_transform(item: font_awesome.Item) -> Tensor:\n",
    "            image = val_transform(item)\n",
    "            return image\n",
    "\n",
    "        self.train_ds = self.train_ds.map(train_transform)\n",
    "        self.val_ds = self.val_ds.map(val_transform)\n",
    "        \n",
    "        self.in_channels = 3\n",
    "        self.num_values = 256\n",
    "        \n",
    "    def train_loader(self, batch_size: int):\n",
    "        return data.DataLoader(\n",
    "            dataset=self.train_ds,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "    \n",
    "    def val_loader(self, batch_size: int):\n",
    "        return data.DataLoader(\n",
    "            dataset=self.val_ds,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 32\n",
    "        self.val_batch_size = self.batch_size\n",
    "        self.num_epochs = 32\n",
    "        self.num_val_samples = 8\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        \n",
    "        self.writer = tensorboard.SummaryWriter()\n",
    "    \n",
    "    def train(self, pix: PixelCNN, pix_data: PixelCNNData):\n",
    "        pix = pix.to(self.device)\n",
    "        \n",
    "        train_loader = pix_data.train_loader(self.batch_size)\n",
    "        val_loader = pix_data.val_loader(self.val_batch_size)\n",
    "        \n",
    "        optim = torch.optim.Adam(pix.parameters(), lr=1e-3)\n",
    "        \n",
    "        pbar = tqdm()\n",
    "        \n",
    "        step_idx = 0\n",
    "        for epoch in range(self.num_epochs):\n",
    "            pbar.reset()\n",
    "            pbar.set_description(f\"Train #{epoch}\")\n",
    "            pbar.total = len(train_loader)\n",
    "            \n",
    "            for images in train_loader:\n",
    "                images = images.to(self.device)\n",
    "                value_dist: Categorical = pix(images)\n",
    "                targets = (255 * images).int()\n",
    "                loss: Tensor = -value_dist.log_prob(targets).mean()\n",
    "                \n",
    "                optim.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            \n",
    "                self.writer.add_scalar(\"train/loss\", loss, global_step=step_idx)\n",
    "                step_idx += len(images)\n",
    "                pbar.update()\n",
    "            \n",
    "            pbar.reset()\n",
    "            pbar.set_description(f\"Val #{epoch}\")\n",
    "            pbar.total = len(val_loader)\n",
    "            \n",
    "            val_grid = []\n",
    "            val_loss = 0\n",
    "            with eval_ctx(pix):\n",
    "                for images in val_loader:\n",
    "                    images = images.to(self.device)\n",
    "                    value_dist: Categorical = pix(images)\n",
    "                    targets = (255 * images).int()\n",
    "                    batch_loss: Tensor = -value_dist.log_prob(targets).mean()\n",
    "                    val_loss += batch_loss * len(images)\n",
    "                    \n",
    "                    if len(val_grid) < self.num_val_samples:\n",
    "                        rem = self.num_val_samples - len(val_grid)\n",
    "                        rem = min(len(images), rem)\n",
    "                        h, w = images.shape[-2:]\n",
    "                        preds = pix.predict(images[:rem], (w//2, h//2))\n",
    "                        for image, pred in zip(images[:rem], preds):\n",
    "                            val_grid.extend([image, pred])\n",
    "                    \n",
    "                    pbar.update()\n",
    "                \n",
    "                self.writer.add_scalar(\"val/loss\", val_loss, global_step=epoch)\n",
    "                pbar.set_postfix({\"val/loss\": val_loss})\n",
    "                \n",
    "                val_grid = visual.make_grid(val_grid, ncols=4)\n",
    "                self.writer.add_image(\"val/samples\", tv_F.to_tensor(val_grid),\n",
    "                                      global_step=epoch)\n",
    "                \n",
    "\n",
    "pix_data = PixelCNNData()\n",
    "pix = PixelCNN(pix_data.in_channels, pix_data.num_values,\n",
    "               num_hidden_layers=8, hidden_dim=64, kernel_size=5)\n",
    "trainer = Trainer()\n",
    "\n",
    "trainer.train(pix, pix_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsrch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
