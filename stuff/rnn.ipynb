{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Union, overload\n",
    "\n",
    "\n",
    "class MaskFactory:\n",
    "    def full(self, kernel_size: Tuple[int, ...]) -> Tensor:\n",
    "        return torch.ones(kernel_size)\n",
    "\n",
    "    def causal(self, kernel_size: Tuple[int, ...]) -> Tensor:\n",
    "        idx = torch.meshgrid([torch.arange(ki) for ki in kernel_size])\n",
    "        idx = torch.cat(idx, -1)\n",
    "        cutoff = torch.tensor([*kernel_size])\n",
    "        return 2 * idx < cutoff\n",
    "\n",
    "\n",
    "class MaskedConv(nn.Module):\n",
    "    def __init__(self, conv: Union[nn.Conv1d, nn.Conv2d], mask: Tensor):\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        self.register_buffer(\"mask\", self.mask)\n",
    "        self.conv = conv\n",
    "        assert self.conv.kernel_size == self.mask.shape\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        self.conv.weight *= self.mask\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, in_features: int, hidden_features: int, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = self.H = hidden_features\n",
    "        self.bias = bias\n",
    "\n",
    "        self.input_map = nn.Linear(in_features, 4 * hidden_features, bias=bias)\n",
    "        self.state_map = nn.Linear(hidden_features, 4 * hidden_features, bias=bias)\n",
    "\n",
    "    def forward(self, x: Tensor, h_c: Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tensor]:\n",
    "        h, c = h_c\n",
    "        gates = self.state_map(h) + self.input_map(x)\n",
    "        i, f, g, o = torch.split(gates, self.H, dim=1)\n",
    "        next_c = torch.sigmoid(f) * c + torch.sigmoid(i) * torch.tanh(g)\n",
    "        next_h = torch.sigmoid(o) * next_c\n",
    "        return next_h, next_c\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, in_features: int, hidden_features: int, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.bias = bias\n",
    "\n",
    "        self.cell = LSTMCell(in_features, hidden_features, bias)\n",
    "\n",
    "    def _forward_tensor(\n",
    "        self, xs: Tensor, h_c: Tuple[Tensor, Tensor]\n",
    "    ) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:\n",
    "        # x.shape = [L, B, N_in]\n",
    "        h, c = h_c  # [B, H]\n",
    "        hs = []\n",
    "        for x in xs:\n",
    "            h, c = self.cell(x, (h, c))\n",
    "            hs.append(h)\n",
    "        hs = torch.stack(hs)\n",
    "        return hs, (h, c)\n",
    "\n",
    "    def _forward_packed(\n",
    "        self, seq: rnn.PackedSequence, h_c: Tuple[Tensor, Tensor]\n",
    "    ) -> Tuple[rnn.PackedSequence, Tuple[Tensor, Tensor]]:\n",
    "        h, c = h_c  # [N, H]\n",
    "        h, c = h[seq.sorted_indices], c[seq.sorted_indices]\n",
    "        hs = torch.empty(len(seq.data), self.hidden_features)\n",
    "        cur_offset = 0\n",
    "        for step_batch in seq.batch_sizes:\n",
    "            cur_slice = slice(cur_offset, cur_offset + step_batch)\n",
    "            h[:step_batch], c[:step_batch] = self.cell(\n",
    "                seq.data[cur_slice], \n",
    "                (h[:step_batch], c[:step_batch]),\n",
    "            )\n",
    "            hs[cur_slice] = h[:step_batch]\n",
    "            cur_offset += step_batch\n",
    "        h_seq = rnn.PackedSequence(\n",
    "            hs, seq.batch_sizes, seq.sorted_indices, seq.unsorted_indices\n",
    "        )\n",
    "        h, c = h[seq.unsorted_indices], c[seq.unsorted_indices]\n",
    "        return h_seq, (h, c)\n",
    "\n",
    "    @overload\n",
    "    def forward(\n",
    "        self, seq: Tensor, h_c: Tuple[Tensor, Tensor]\n",
    "    ) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    def forward(\n",
    "        self, seq: rnn.PackedSequence, h_c: Tuple[Tensor, Tensor]\n",
    "    ) -> Tuple[rnn.PackedSequence, Tuple[Tensor, Tensor]]:\n",
    "        ...\n",
    "\n",
    "    def forward(self, seq, h_c):\n",
    "        if isinstance(seq, Tensor):\n",
    "            return self._forward_tensor(seq, h_c)\n",
    "        elif isinstance(seq, rnn.PackedSequence):\n",
    "            return self._forward_packed(seq, h_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PackedSequence(data=tensor([[ 0.2645, -0.0116,  0.2647,  ...,  0.2017,  0.1374,  0.1409],\n",
       "         [ 0.2498,  0.2412,  0.2512,  ...,  0.2587,  0.2885,  0.0894],\n",
       "         [ 0.3387,  0.0392,  0.1993,  ...,  0.1035,  0.1053,  0.2107],\n",
       "         ...,\n",
       "         [ 0.2644, -0.1400, -0.0008,  ...,  0.0713,  0.0903, -0.1108],\n",
       "         [ 0.2506, -0.1405, -0.1005,  ...,  0.1730,  0.0544, -0.0716],\n",
       "         [ 0.1099, -0.1045,  0.0009,  ...,  0.0914,  0.0951, -0.1583]],\n",
       "        grad_fn=<CopySlices>), batch_sizes=tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7, 5, 5, 4, 4, 3,\n",
       "         3, 3, 3, 2, 1, 1, 1]), sorted_indices=tensor([1, 0, 5, 7, 2, 4, 6, 3]), unsorted_indices=tensor([1, 0, 4, 7, 5, 2, 6, 3])),\n",
       " (tensor([[ 1.8957e-01, -6.5620e-03,  8.8885e-02,  1.5788e-01, -1.3660e-01,\n",
       "            1.3060e-01, -3.2869e-02,  1.6255e-02,  8.9979e-02,  1.7170e-01,\n",
       "            4.5223e-02, -2.2077e-01,  5.3314e-02,  2.5696e-01,  1.8950e-01,\n",
       "            4.0378e-01, -9.6875e-02, -9.3238e-02, -1.3211e-01,  1.2778e-01,\n",
       "            8.9714e-02,  1.2746e-01, -4.9839e-02, -1.4257e-01, -2.0661e-01,\n",
       "           -5.1302e-02,  1.3300e-01,  5.2890e-02,  1.1123e-01, -2.8302e-02,\n",
       "            7.2081e-02, -1.1160e-01],\n",
       "          [ 1.0987e-01, -1.0451e-01,  8.7944e-04,  6.0198e-02, -1.0418e-01,\n",
       "            8.5589e-02, -8.2878e-03,  3.9313e-02,  1.3238e-01,  1.5625e-01,\n",
       "            5.6152e-02, -2.9301e-01,  1.0549e-01,  2.4289e-01,  7.6822e-02,\n",
       "            4.3768e-01, -4.9506e-02, -1.2916e-01, -6.1008e-02,  1.3709e-01,\n",
       "           -1.1420e-02, -5.5959e-02, -2.3937e-01,  1.2583e-01, -1.4874e-01,\n",
       "           -6.4619e-02,  1.4654e-01,  3.6465e-02,  1.6274e-01,  9.1385e-02,\n",
       "            9.5146e-02, -1.5827e-01],\n",
       "          [ 2.2247e-01, -8.0673e-02,  5.2368e-02,  1.1715e-01, -1.2462e-01,\n",
       "            1.3755e-01, -4.9503e-02,  3.3128e-02,  7.0082e-02,  1.9328e-01,\n",
       "            5.3306e-02, -2.4890e-01,  1.3139e-01,  2.9209e-01,  1.6210e-01,\n",
       "            3.9447e-01, -6.3162e-03, -5.9119e-02, -1.5770e-01,  8.8492e-02,\n",
       "            3.9041e-02,  6.1217e-02, -1.6234e-01, -1.2172e-02, -2.8400e-01,\n",
       "           -3.7784e-02,  9.7504e-02,  7.6687e-02,  1.8186e-01, -9.3555e-04,\n",
       "            2.0550e-02, -6.3798e-02],\n",
       "          [ 1.1981e-01, -7.1798e-02,  4.6800e-02,  1.1596e-01, -9.3124e-02,\n",
       "            7.8642e-02, -9.3215e-03,  3.4908e-02,  1.0728e-01,  1.9327e-01,\n",
       "            8.4148e-02, -3.4110e-01,  1.1826e-01,  3.3428e-01,  7.1220e-02,\n",
       "            4.8954e-01, -9.4137e-03, -8.8121e-02, -1.1309e-01,  1.3862e-01,\n",
       "            7.2139e-03,  7.3287e-03, -1.6875e-01,  6.2178e-02, -3.0041e-01,\n",
       "           -1.9794e-02,  1.3044e-01,  1.0767e-01,  1.4539e-01, -7.1398e-02,\n",
       "            2.3673e-02, -1.5857e-01],\n",
       "          [ 2.3677e-01, -1.4108e-01,  5.5009e-02,  7.8127e-02, -1.6142e-01,\n",
       "            6.5205e-02, -9.5463e-02,  2.2056e-02,  7.0068e-02,  1.8879e-01,\n",
       "            3.4619e-02, -2.3303e-01,  7.9922e-02,  2.8989e-01,  1.4653e-01,\n",
       "            3.3089e-01, -8.9761e-02, -1.1448e-01, -1.1170e-01,  1.4875e-01,\n",
       "            2.0460e-01, -7.5061e-02, -2.1005e-01, -3.9245e-03, -2.0862e-01,\n",
       "           -7.5119e-02,  7.5242e-02,  1.4382e-02,  8.8653e-02, -3.4092e-02,\n",
       "            9.8859e-02, -1.1489e-01],\n",
       "          [ 2.0495e-01, -1.5782e-01,  3.5698e-02,  9.5554e-02, -1.1159e-01,\n",
       "            1.2289e-01, -1.4771e-02,  8.2045e-02,  1.2348e-01,  2.1430e-01,\n",
       "            4.2676e-04, -2.4543e-01,  7.6502e-02,  2.2143e-01,  1.6573e-01,\n",
       "            3.9834e-01, -6.0190e-02, -1.9123e-01, -1.1328e-01,  1.1156e-01,\n",
       "            1.4708e-01,  6.6152e-03, -1.8974e-01, -4.1630e-03, -2.0072e-01,\n",
       "           -3.5312e-02,  1.2389e-01, -8.3858e-03,  1.3297e-01, -2.8191e-03,\n",
       "            7.3497e-02,  2.6257e-03],\n",
       "          [ 2.5935e-01, -5.7278e-02, -8.1138e-03,  1.1132e-01, -1.4203e-01,\n",
       "            9.8275e-02, -1.5660e-02,  5.7476e-03,  6.2957e-02,  1.8671e-01,\n",
       "            6.5036e-02, -1.1660e-01,  1.3406e-01,  2.7190e-01,  1.6313e-01,\n",
       "            3.4447e-01, -4.8508e-02, -6.7826e-02, -8.1956e-02,  1.6583e-01,\n",
       "            1.5480e-01, -3.1792e-02, -2.6931e-01, -2.5046e-03, -2.3149e-01,\n",
       "           -9.3881e-02,  1.0177e-01, -1.7130e-02,  1.6455e-01,  1.3806e-01,\n",
       "            8.3177e-02, -4.9257e-02],\n",
       "          [ 1.5535e-01,  1.9663e-02,  9.1568e-02,  1.4350e-01, -1.3779e-01,\n",
       "            9.4873e-02, -3.3687e-02,  6.7083e-02,  1.0750e-01,  1.8433e-01,\n",
       "            5.6411e-02, -2.2154e-01,  7.8773e-02,  2.4315e-01,  1.7556e-01,\n",
       "            3.3072e-01, -3.1521e-02, -8.7622e-02, -6.8565e-02,  1.6816e-01,\n",
       "            1.2060e-01, -7.0378e-02, -2.5002e-01, -4.6972e-02, -2.5973e-01,\n",
       "           -9.3571e-02,  1.6215e-01, -1.9588e-02,  1.8674e-01,  3.8734e-02,\n",
       "            1.0817e-01, -1.4512e-01]], grad_fn=<IndexBackward0>),\n",
       "  tensor([[ 0.3455, -0.0135,  0.1475,  0.2848, -0.3325,  0.2610, -0.0548,  0.0349,\n",
       "            0.2274,  0.2952,  0.1022, -0.3262,  0.1258,  0.4492,  0.2977,  0.8037,\n",
       "           -0.1971, -0.1584, -0.2731,  0.2290,  0.1633,  0.1756, -0.1157, -0.2161,\n",
       "           -0.3260, -0.1358,  0.3032,  0.1063,  0.1813, -0.0383,  0.2822, -0.1795],\n",
       "          [ 0.2217, -0.2636,  0.0016,  0.1313, -0.2297,  0.1663, -0.0154,  0.0658,\n",
       "            0.3447,  0.2543,  0.1395, -0.4574,  0.1989,  0.4302,  0.1210,  0.7204,\n",
       "           -0.0838, -0.2435, -0.1314,  0.2670, -0.0230, -0.0883, -0.5245,  0.2270,\n",
       "           -0.2322, -0.1762,  0.3268,  0.0847,  0.2738,  0.1397,  0.2110, -0.2883],\n",
       "          [ 0.3636, -0.2199,  0.1024,  0.2493, -0.3189,  0.2785, -0.0861,  0.0606,\n",
       "            0.1676,  0.3340,  0.1052, -0.3793,  0.2957,  0.4604,  0.2478,  0.7718,\n",
       "           -0.0115, -0.0983, -0.3276,  0.1865,  0.0700,  0.0843, -0.3506, -0.0212,\n",
       "           -0.4424, -0.1043,  0.2453,  0.1759,  0.2925, -0.0013,  0.0728, -0.1063],\n",
       "          [ 0.2254, -0.1812,  0.0773,  0.2385, -0.2307,  0.1598, -0.0173,  0.0582,\n",
       "            0.2680,  0.3280,  0.1945, -0.5031,  0.2577,  0.5182,  0.1077,  0.8812,\n",
       "           -0.0173, -0.1655, -0.2631,  0.3084,  0.0137,  0.0101, -0.3928,  0.1018,\n",
       "           -0.4852, -0.0653,  0.2739,  0.2257,  0.2156, -0.1043,  0.0753, -0.2862],\n",
       "          [ 0.3853, -0.3900,  0.1075,  0.1562, -0.3776,  0.1423, -0.1852,  0.0394,\n",
       "            0.1607,  0.3334,  0.0777, -0.3885,  0.1669,  0.4328,  0.2271,  0.6834,\n",
       "           -0.1474, -0.1812, -0.2333,  0.3242,  0.3825, -0.1105, -0.3911, -0.0069,\n",
       "           -0.3491, -0.1941,  0.1862,  0.0364,  0.1427, -0.0464,  0.2637, -0.1867],\n",
       "          [ 0.3481, -0.4000,  0.0621,  0.1822, -0.2613,  0.3043, -0.0254,  0.1351,\n",
       "            0.3207,  0.3515,  0.0009, -0.3857,  0.1781,  0.3652,  0.2591,  0.7280,\n",
       "           -0.1029, -0.3173, -0.2657,  0.2385,  0.2394,  0.0099, -0.4720, -0.0066,\n",
       "           -0.3087, -0.1183,  0.2848, -0.0178,  0.2327, -0.0039,  0.2173,  0.0041],\n",
       "          [ 0.4213, -0.1459, -0.0168,  0.1904, -0.3645,  0.1743, -0.0308,  0.0109,\n",
       "            0.1579,  0.2880,  0.1373, -0.2024,  0.2636,  0.4023,  0.2746,  0.6594,\n",
       "           -0.0834, -0.1305, -0.1987,  0.3626,  0.2783, -0.0441, -0.5401, -0.0040,\n",
       "           -0.3906, -0.2878,  0.2198, -0.0355,  0.2706,  0.1855,  0.2549, -0.0781],\n",
       "          [ 0.2988,  0.0462,  0.1526,  0.2660, -0.3543,  0.1962, -0.0639,  0.1376,\n",
       "            0.2480,  0.2901,  0.1400, -0.3570,  0.1568,  0.3977,  0.2969,  0.6974,\n",
       "           -0.0527, -0.1706, -0.1376,  0.3668,  0.2208, -0.1001, -0.4804, -0.0735,\n",
       "           -0.4214, -0.2716,  0.3324, -0.0439,  0.3037,  0.0508,  0.3125, -0.2305]],\n",
       "         grad_fn=<IndexBackward0>)))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 8\n",
    "gen = torch.Generator().manual_seed(0)\n",
    "lengths = torch.randint(16, 32, (N,), generator=gen)\n",
    "seq_list = [torch.rand((seq_len, 16), generator=gen) for seq_len in lengths]\n",
    "seq = rnn.pack_sequence(seq_list, enforce_sorted=False)\n",
    "\n",
    "lstm = LSTM(16, 32)\n",
    "h, c = torch.rand((N, 32), generator=gen), torch.rand((N, 32), generator=gen)\n",
    "lstm(seq, (h, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan]], grad_fn=<MulBackward0>),\n",
       " tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell = LSTMCell(16, 32)\n",
    "x = torch.empty(1, 16)\n",
    "h, c = torch.empty(1, 32), torch.empty(1, 32)\n",
    "cell(x, (h, c))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('rsrch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b948e45c948dd176e3efe7e1cfd0e5e7237983d8db14ea5811853abda2d845c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
